---
title: 大数据整合
categories:
  - 大数据
tags:
  - 大数据
date: 2020-08-05 14:11:33
permalink: /pages/950f4c/
---
- 1. 准备
- 2. Zookeeper
- 3. Hadoop HA
- 4. Hbase
- 5. Hbase 集成 phoenix
- 6. Hive
- 7. Spark
- 8. Kylin
- 9. Grafana
- 10. Web UI
- 11. 集成 Kerberos 认证 （复杂）
- 12. 扩展

<!-- more -->

注：带 <> 符号的参数需要连带 <> 符号一起替换为实际环境的参数

## 1. 准备
节点 hostname | IP
-- | --
node1 | 172.*.*.27
node2 | 172.*.*.28
node3 | 172.*.*.29

### 1.1 hadoop用户和组

```
groupadd hadoop
useradd hadoop -g hadoop

修改密码
passwd hadoop

mkdir -p /opt/pkg
chown -R hadoop:hadoop /opt/pkg

切换至 hadoop 用户
su hadoop
```

### 1.2 JDK
下载
```
https://www.oracle.com/technetwork/java/javase/downloads/index.html
```
解压
```
tar -xf jdk-13.0.2_linux-x64_bin.tar.gz -C /opt/pkg

cd /opt/pkg
ln -s jdk-13.0.2 jdk
```
拷贝到其他机器
```
scp -r jdk 172.*.*.28:/opt/pkg/
scp -r jdk 172.*.*.29:/opt/pkg/
```

环境变量
```
vi /etc/profile

# jdk
export JAVA_HOME=/opt/pkg/jdk
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
```
```
source /etc/profile
java -version
```

### 1.3 Firewall
```
systemctl stop firewalld.service
```

### 1.4 hosts 映射 hostname
注意：映射的是 hostname，因为 zookeeper 存储的是 hostname 而不是 IP
```
vi /etc/hosts
```
```
172.*.*.27  node1
172.*.*.28  node2
172.*.*.29  node3
```

### 1.5 免密登录
```
su hadoop

ssh-keygen
三次回车

ssh-copy-id 172.*.*.27
ssh-copy-id node1

ssh-copy-id 172.*.*.28
ssh-copy-id node2

ssh-copy-id 172.*.*.29
ssh-copy-id node3
```

### 1.6 时间同步
```
yum install -y ntpdate
```
自动同步
```
crontab -e

添加
0 * * * * ntpdate -u 172.*.*.99 > /dev/null 2>&1
```
开机同步
```
chmod +x /etc/rc.local

vi /etc/rc.local

添加
ntpdate -u 172.*.*.99 > /dev/null 2>&1
```




## 2. Zookeeper

 节点 | IP
 --- | ---
 node1 | 172.*.*.27
 node2 | 172.*.*.28
 node3 | 172.*.*.29

### 2.1 下载

下载地址
```
http://zookeeper.apache.org/releases.html

https://www.apache.org/dyn/closer.cgi/zookeeper/
```

下载命令
```
wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz
```

### 2.2 解压
```
tar -xf apache-zookeeper-3.5.6-bin.tar.gz -C /opt/pkg

cd /opt/pkg/
ln -s apache-zookeeper-3.5.6-bin zookeeper
```

### 2.3 配置

#### （1） zoo.cfg
```
cp /opt/pkg/zookeeper/conf/zoo_sample.cfg /opt/pkg/zookeeper/conf/zoo.cfg
vi /opt/pkg/zookeeper/conf/zoo.cfg
```

**配置：**

```
# 基本事件单元，以毫秒为单位，这个时间作为 Zookeeper 服务器之间或客户端之间维持心跳的时间间隔
tickTime=2000

# 最大心跳时间 initLimit 乘以 tickTime
initLimit=10

# 这个配置项表示 主从 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度
syncLimit=5

# 数据目录
dataDir=/data/zookeeper

# 服务端口
clientPort=2181

# 管理端口，默认8080
admin.serverPort=8888

# 集群节点，2888原子广播端口，3888选举端口
server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888
```

**同步配置**

```
scp -r /opt/pkg/zookeeper/* node2:/opt/pkg/zookeeper/
scp -r /opt/pkg/zookeeper/* node3:/opt/pkg/zookeeper/

同步之后所有机器执行
chown hadoop:hadoop -R /opt/pkg/zookeeper
```

#### （2）数据目录

```
mkdir -p /data/zookeeper

chown hadoop:hadoop -R /data/zookeeper
```


#### （3）myid
```
所有节点，内容与zoo.cfg的 server.* 对应
vi /data/zookeeper/myid
```

IP1 节点的 /data/zookeeper/myid 的内容为
```
1
```

IP2 节点的 /data/zookeeper/myid 的内容为
```
2
```

IP3 节点的 /data/zookeeper/myid 的内容为
```
3
```

#### （4）注册服务

```
vi /usr/lib/systemd/system/zookeeper.service
```

内容：

```
[Unit]
Description=zookeeper service
After=syslog.target network.target

[Service]
Type=forking
Environment=ZOO_LOG_DIR=/opt/pkg/zookeeper/logs
Environment=JAVA_HOME=/opt/pkg/jdk
ExecStart=/opt/pkg/zookeeper/bin/zkServer.sh start
ExecStop=/opt/pkg/zookeeper/bin/zkServer.sh stop
User=hadoop
Group=hadoop
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
```

```
systemctl daemon-reload
```

#### （5）环境变量
```
vi /etc/profile

# zookeeper
export ZOOKEEPER_HOME=/opt/pkg/zookeeper
export PATH=$ZOOKEEPER_HOME/bin:$PATH
```
```
source /etc/profile
zkServer.sh status
```

### 2.4 启动

```
systemctl start zookeeper
```

**开机自启**

```
systemctl enable zookeeper
```

### 2.5 测试

```
zkServer.sh status
```

## 3. Hadoop HA


节点 | IP | 组件
-- | -- | --
node1 | 172.*.*.27 | NameNode、JournalNode、DFSZKFailoverController、ResourceManager、JobHistoryServer
node2 | 172.*.*.28 | NameNode、DataNode、JournalNode、DFSZKFailoverController、ResourceManager
node3 | 172.*.*.29 | DataNode、JournalNode


### 3.1 下载
```
https://hadoop.apache.org/releases.html
```


### 3.2 解压
```
tar -xf hadoop-3.2.1.tar.gz -C /opt/pkg

cd /opt/pkg
ln -s hadoop-3.2.1 hadoop

chown hadoop:hadoop -R /opt/pkg
```


### 3.3 配置
#### （1）hdfs-site.xml
vi /opt/pkg/hadoop/etc/hadoop/hdfs-site.xml
```
<configuration>

        <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的fs.defaultFS对应 -->
        <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
        </property>

        <!-- mycluster下面有两个NameNode，分别是namenode1，namenode2, 名字可自定义, 注意与通信地址配置一致即可 -->
        <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>namenode1,namenode2</value>
        </property>

        <!-- namenode1的RPC通信地址, 注意name最后两个名称 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode1</name>
                <value>node1:9000</value>
        </property>

        <!-- namenode1的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode1</name>
                <value>node1:50070</value>
        </property>

        <!-- namenode2的RPC通信地址 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode2</name>
                <value>node2:9000</value>
        </property>
        <!-- namenode2的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode2</name>
                <value>node2:50070</value>
        </property>

        <!-- 设置一组 journalNode 的 URI 地址，active NameNode 将 edit log 写入这些JournalNode -->
        <!-- 而 standby NameNode 读取这些 edit log，并作用在内存中的目录树中。-->
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://node1:8485;node2:8485;node3:8485/mycluster</value>
        </property>

        <!-- JournalNode 存储路径 -->
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/data/hadoop/journal/</value>
        </property>

        <!-- 数据块大小，默认为64M -->
        <property>
                <name>dfs.block.size</name>
                <value>134217728</value>
        </property>

        <!-- 对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。-->
        <!-- 设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。-->
        <!-- 疑问: 集群大小是如何定义的?-->
        <property>
                <name>dfs.namenode.handler.count</name>
                <value>10</value>
        </property>

        <!-- DataNode 存储路径 -->
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/data/hadoop/data/</value>
        </property>

        <!-- 块副本数 -->
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>

        <!-- 关闭权限管理 -->
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>

        <!-- NameNode 存储路径 -->
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/data/hadoop/name/</value>
        </property>

        <!-- 开启NameNode故障时自动切换 -->
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>

        <!-- 指定mycluster（与最上方dfs.nameservices配置对应）出故障时，哪个实现类负责执行故障切换，注意：mycluster一定要与nameservices配置一致 -->
        <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>

        <!-- 发生故障时，避免两个NameNode都为Active状态，使用ssh方式kill掉一个 -->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>
                    sshfence
                    shell(/bin/true)
                </value>
        </property>

        <!-- 配置sshfence使用的私钥路径 -->
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/home/hadoop/.ssh/id_rsa</value>
        </property>

        <!-- 配置sshfence超时时长 -->
        <property>
                <name>dfs.ha.fencing.ssh.connect-timeout</name>
                <value>5000</value>
        </property>

        <!-- 使用IP配置 -->
        <property>
                <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
                <value>false</value>
        </property>

</configuration>
```

#### （2）core-site.xml
vi /opt/pkg/hadoop/etc/hadoop/core-site.xml
```
<configuration>
    <!-- 指定hdfs的nameservice为node，需要和hdfs-site.xml中的dfs.nameservices一致 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://mycluster</value>
    </property>

    <!--指定hadoop数据临时存放目录-->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop/tmp</value>
    </property>

    <!--指定hdfs操作数据的缓冲区大小 可以不配-->
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>

    <!--指定zookeeper地址-->
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
</configuration>
```

#### （3）yarn-site.xml
vi /opt/pkg/hadoop/etc/hadoop/yarn-site.xml
```
<configuration>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>node1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>node2</value>
    </property>
    <!--开启故障自动切换-->
    <property>
       <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
       <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>node1:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>node2:8088</value>
    </property>

    <!--配置与zookeeper的连接地址-->
    <property>
      <name>yarn.resourcemanager.zk-state-store.address</name>
      <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>

    <!--开启自动恢复功能-->
    <property>
      <name>yarn.resourcemanager.recovery.enabled</name>
      <value>true</value>
    </property>


    <property>
      <name>yarn.nodemanager.pmem-check-enabled</name>
      <value>false</value>
    </property>
    <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
    </property>
    <!-- 指定nodemanager启动时加载server的方式为shuffle server -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.application.classpath</name>
        <value>/opt/pkg/hadoop/etc/hadoop,/opt/pkg/hadoop/share/hadoop/mapreduce/*,/opt/pkg/hadoop/share/hadoop/mapreduce/lib/*,/opt/pkg/hadoop/share/hadoop/common/*,/opt/pkg/hadoop/share/hadoop/common/lib/*,/opt/pkg/hadoop/share/hadoop/hdfs/*,/opt/pkg/hadoop/share/hadoop/hdfs/lib/*,/opt/pkg/hadoop/share/hadoop/yarn/*,/opt/pkg/hadoop/share/hadoop/yarn/lib/*</value>
    </property>
</configuration>
```

#### （4）hadoop-env.sh
vi /opt/pkg/hadoop/etc/hadoop/hadoop-env.sh
```
# 配置JAVA_HOME
export JAVA_HOME=/opt/pkg/jdk
```

#### （5）workers
**dataNode节点**
vi /opt/pkg/hadoop/etc/hadoop/workers
```
172.*.*.28
172.*.*.29
```

#### （6）mapred-site.xml
vi /opt/pkg/hadoop/etc/hadoop/mapred-site.xml
```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node1:10020</value>
    </property>

    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node1:19888</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/history/done</value>
    </property>
    <property>
        <name>mapreudce.jobhistory.intermediate.done-dir</name>
        <value>/history/done/done_intermediate</value>
    </property>

    <property>
        <name>mapreduce.application.classpath</name>
        <value>/opt/pkg/hadoop/etc/hadoop,/opt/pkg/hadoop/share/hadoop/mapreduce/*,/opt/pkg/hadoop/share/hadoop/mapreduce/lib/*,/opt/pkg/hadoop/share/hadoop/common/*,/opt/pkg/hadoop/share/hadoop/common/lib/*,/opt/pkg/hadoop/share/hadoop/hdfs/*,/opt/pkg/hadoop/share/hadoop/hdfs/lib/*,/opt/pkg/hadoop/share/hadoop/yarn/*,/opt/pkg/hadoop/share/hadoop/yarn/lib/*</value>
    </property>
</configuration>
```

#### （6）同步配置
```
node2、node3节点创建hadoop目录:
mkdir -p /opt/pkg/hadoop/

su hadoop
scp -r /opt/pkg/hadoop/* node2:/opt/pkg/hadoop/
scp -r /opt/pkg/hadoop/* node3:/opt/pkg/hadoop/
```

#### （7）环境变量
```
vi /etc/profile
```
```
# hadoop
export HADOOP_HOME=/opt/pkg/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```
```
source /etc/profile
```


### 3.4 启动
#### （1）创建数据目录
```
mkdir -p /data/hadoop/name
mkdir /data/hadoop/data
mkdir /data/hadoop/tmp
mkdir /data/hadoop/journal
mkdir /opt/pkg/hadoop/logs

chown -R hadoop:hadoop /data/hadoop/
```

#### （2）启动 journalnode
```
所有 journalnode 节点启动
hdfs --daemon start journalnode

启动后查看日志是否有错误
```

#### （3）格式化
其中一个 nameNode 运行
```
hadoop namenode -format
查看 journalnode 数据目录是否有数据
ll /data/hadoop/journal/

启动一个 nameNode
hdfs --daemon start namenode
```
另一个 nameNode 节点运行
```
hdfs namenode -bootstrapStandby
```

#### （4）格式化 zookeeper
```
hdfs zkfc -formatZK
```

#### （5）启动全部
```
start-all.sh

启动 yarn job history
mr-jobhistory-daemon.sh start historyserver
```

### 3.5 验证
```
使用 jps 命令查看各个组件是否在运行
node1 节点：
    7632 DFSZKFailoverController
    8436 ResourceManager
    7447 NameNode
    6748 QuorumPeerMain
    9373 Jps
    7022 JournalNode

node2 节点:
    26178 JournalNode
    26434 NameNode
    26020 QuorumPeerMain
    26345 DFSZKFailoverController
    27131 Jps
    26524 DataNode

node3 节点:
    18294 JournalNode
    18413 DataNode
    18782 Jps
    18143 QuorumPeerMain


访问
    http://node1:50070
    http://node2:50070
    查看NameNode是否一个为active，另一个为standby

    在页面上选择上方DataNodes查看datanode节点是否齐全

使用简单的hdfs命令
    hdfs dfs -fs hdfs://node1:9000 -ls /
    hdfs dfs -fs hdfs://node1:9000 -mkdir /test

    注意node1要替换为active状态的NameNode，不能用standby状态的

上传文件
    hdfs dfs -fs hdfs://node1:9000 -put test1.txt /test
```
**HA 验证**
```
关闭active状态的 namenode 节点
hdfs --daemon stop namenode

查看另一个 namenode 是否变为 active
再执行简单的 hdfs 命令验证
```

### 3.6 问题
#### （1）Web UI 中 Live Nodes 数量和 Datanodes 页面显示数量不一致
```
在 /etc/hosts 文件添加 ip 映射，用映射名称配置，并重启电脑
```

#### （2）Mismatched address stored in ZK for NameNode
java.lang.RuntimeException: Mismatched address stored in ZK for NameNode at /172.*.*.28:9000: Stored protobuf was nameserviceId: "mycluster"
```
zookeeper 存储的 hostname 与我们的 namenode 不一致导致的，因为zk中只存储域名，而在我们当前主机是ping不同对应的主机，故而出现此错误。解决方法是在/etc/hosts文件中添加所有主机的映射信息。
```





## 4. Hbase
| 节点 | IP | 组件
| --- | --- | --
| node1 | 172.*.*.27 | master
| node2 | 172.*.*.28 | regionserver
| node3 | 172.*.*.29 | regionserver
### 4.1 下载
```
http://hbase.apache.org/downloads.html
```

### 4.2 解压
```
tar -xf hbase-2.2.3-bin.tar.gz -C /opt/pkg

cd /opt/pkg
ln -s hbase-2.2.3 hbase
```

### 4.3 配置
#### （1）hbase-env.sh
vi /opt/pkg/hbase/conf/hbase-env.sh
```
# JDK
export JAVA_HOME=/opt/pkg/jdk

# hbase配置文件的路径
export HBASE_CLASSPATH=/opt/pkg/hbase/conf

# 此配置信息，设置由独立的zk集群管理，故为false
export HBASE_MANAGES_ZK=false

# Hbase日志目录
export HBASE_LOG_DIR=/opt/pkg/hbase/logs

# Hbase pid 目录
export HBASE_PID_DIR=/var/hadoop/pids
```

#### （2）hbase-site.xml
vi /opt/pkg/hbase/conf/hbase-site.xml
```
<configuration>
    <!-- 指定hbase在HDFS上存储的路径，注意需要与hadoop的配置相对于 -->
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://mycluster/data/hbase</value>
    </property>

    <!-- 指定hbase是分布式的 -->
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>

    <!-- master节点 --> <!-- 个人理解，多master节点时，应该是不需要配置此项亦可，通过zookeeper实现服务发现 -->
    <!--
    <property>
        <name>hbase.master.hostname</name>
        <value>node1</value>
    </property>
    -->

    <!-- 指定zookeeper的地址，多个用“,”分割 -->
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>node1,node2,node3</value>
    </property>

    <!-- 指定zookeeper端口 -->
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>

    <!-- Master绑定的端口，包括backup-master -->
    <property>
        <name>hbase.master.port</name>
        <value>16000</value>
    </property>

    <!-- zookeeper存储路径 -->
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/data/zookeeper</value>
    </property>

    <!-- 临时文件存储路径 -->
    <property>
        <name>hbase.tmp.dir</name>
        <value>/data/hbase/tmp</value>
    </property>

    <property>
        <name>zookeeper.znode.parent</name>
        <value>/hbase</value>
    </property>

    <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
    </property>

    <property>
        <name>hbase.master.info.port</name>
        <value>60010</value>
    </property>

    <!-- HRegionServer 频繁宕掉时配置 -->
    <property>
        <name>hbase.coprocessor.abortonerror</name>
        <value>false</value>
    </property>
</configuration>
```
附加参数调优
```
    <!-- 主表更新操作建立索引的最大线程数，默认为 10 -->
    <property>
        <name>index.builder.threads.max</name>
        <value>10</value>
    </property>

    <!-- 主表更新操作建立索引的最大超时时间，默认 60 秒 -->
    <property>
        <name>index.builder.threads.keepalivetime</name>
        <value>10</value>
    </property>

    <!-- 将索引写到索引表的最大线程数，默认为 10 -->
    <property>
        <name>index.writer.threads.max</name>
        <value>10</value>
    </property>

    <!-- 将索引写到索引表的最大超时时间，默认 60 秒 -->
    <property>
        <name>index.writer.threads.keepalivetime</name>
        <value>60</value>
    </property>

    <!-- 同时往索引表写入数据的最大线程数，默认 2,147,483,647 -->
    <property>
        <name>hbase.htable.threads.max</name>
        <value>2,147,483,647</value>
    </property>

    <!-- 同时往索引表写入数据的最大线程数，默认 60 秒 -->
    <property>
        <name>hbase.htable.threads.keepalivetime</name>
        <value>60</value>
    </property>

    <!-- 缓存 10 个往索引表写数据的线程，默认 10 -->
    <property>
        <name>index.tablefactory.cache.size</name>
        <value>10</value>
    </property>
```



#### （3）regionservers
regionservers是HBase中最主要的组件，负责table数据的实际读写，管理Region。在分布式集群中，HRegionServer一般跟DataNode在同一个节点上

vi /opt/pkg/hbase/conf/regionservers
```
172.*.*.28
172.*.*.29
```

#### （4）链接hadoop 配置
```
cd /opt/pkg/hbase/conf/
ln -s /opt/pkg/hadoop/etc/hadoop/hdfs-site.xml hdfs-site.xml
ln -s /opt/pkg/hadoop/etc/hadoop/core-site.xml core-site.xml
```

#### （5）同步配置
```
node2、node3 节点创建 hbase 目录:
mkdir -p /opt/pkg/hbase/
chown hadoop:hadoop -R /opt/pkg/hbase/

scp -r /opt/pkg/hbase/*  node2:/opt/pkg/hbase/
scp -r /opt/pkg/hbase/*  node3:/opt/pkg/hbase/
```

#### （6）配置环境变量
```
vi /etc/profile
```
```
# hbase
export HBASE_HOME=/opt/pkg/hbase
export PATH=$HBASE_HOME/bin:$PATH
```
```
source /etc/profile
```

### 4.4 启动
```
创建目录
mkdir -p /data/hbase/tmp
mkdir -p /var/hadoop/pids
chown hadoop:hadoop -R /data/hbase/tmp
chown hadoop:hadoop -R /var/hadoop/pids

启动
start-hbase.sh
```

### 4.5 验证
访问 web ui
```
http://node1:60010
查看 Region Servers 节点是否正确
```

**hbase shell 验证**
```
[hadoop@node1 conf]$ hbase shell
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/pkg/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/pkg/hbase-2.2.3/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
HBase Shell
Use "help" to get list of supported commands.
Use "exit" to quit this interactive shell.
For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell
Version 2.2.3, r6a830d87542b766bd3dc4cfdee28655f62de3974, 2020年 01月 10日 星期五 18:27:51 CST
Took 0.0037 seconds
  ase(main):001:0> list
▽ABLE
0 row(s)
Took 0.5823 seconds
=> []
hbase(main):002:0> create 'test','name','info'
Created table test
Took 2.4881 seconds
=> Hbase::Table - test
hbase(main):003:0>
hbase(main):004:0*
hbase(main):005:0* list
TABLE
test
1 row(s)
Took 0.0103 seconds
=> ["test"]
hbase(main):006:0> desc 'test'
Table test is ENABLED
test
COLUMN FAMILIES DESCRIPTION
{NAME => 'id', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE
_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER
=> 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false',
COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}

{NAME => 'info', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CAC
HE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTE
R => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false'
, COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}

2 row(s)

QUOTAS
0 row(s)
Took 0.5302 seconds
# 插入数据，以备后面测试使用
hbase(main):011:0> put 'test','1','name:first','zhang'
Took 0.0870 seconds

hbase(main):012:0> put 'test','1','name:last','zhicheng'
Took 0.0102 seconds

hbase(main):013:0> put 'test','1','info:age','100'
Took 0.0250 seconds
hbase(main):016:0> put 'test','1','info:gender','男'
Took 0.0152 seconds
hbase(main):008:0> exit
```

### 4.6 常用 hbase shell
```
# 获得某一个命令的详细信息
help 'command_name'
# e.g
help 'list'

# 查询服务器状态
status

# 查询Hbase版本
version

# 查看所有表
list

# 创建表
create 'tableName','列族1','列族2'....
# e.g
create 'test','name','other_info'

# 获得表的描述
describe 'tableName'
# e.g
desc 'tableName'

# 添加列族
alter 'tableName','列族名'
# e.g
alter 'member', 'id'

# 删除列族
alert 'tableName', {NAME => '列族名', METHOD => 'delete'}
# e.g
alter 'member', {NAME => 'member_id', METHOD => 'delete’}

# 删除列
（1）delete命令
delete 'tableName','rowKey','列族:列标识'
# e.g
delete 'member','debugo','info:age'

（2）删除整行的值：deleteall
deleteall 'tableName','rowKey'
# e.g
deleteall 'member','debugo'

# 通过enable和disable来启用/禁用这个表,相应的可以通过is_enabled和is_disabled来检查表是否被禁用。
enable 'tableName'
disable 'tableName'
is_enabled 'member'
is_disabled 'member'

# 检查表是否存在
exists 'member'

# 删除表，需要先将表disable
disable 'member'
drop 'member'

# put
在HBase shell中，我们可以通过put命令来插入数据。例如我们新创建一个表，它拥有id、address和info三个列簇，并插入一些数据。列簇下的列不需要提前创建，在需要时通过:来指定即可。

create 'test','address','info'
# 数据
put 'member', 'debugo','info:age','27'
put 'member', 'debugo','info:birthday','1987-04-04'


# 查询
# 查询表中有多少行：count
count 'tableName'
# e.g
count 'member'

# get
（1）获取一个rowKey的所有数据
get 'tableName', 'rowKey'
# e.g
get 'member', 'Sariel'

（2）获得一个rowKey，一个列簇（一个列）中的所有数据
get 'tableName', 'rowKey' , '列族'
get 'tableName', 'rowKey' , '列族:列标识'
# e.g
get 'member', 'Sariel', 'info'

# 查询整表数据
scan 'tableName'
# e.g
scan 'member'
```





## 5. Hbase 集成 phoenix
**环境要求**
- JDK 8
- Python 2.7

### 5.1 下载
```
http://archive.apache.org/dist/phoenix/apache-phoenix-5.0.0-HBase-2.0/bin/apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz
```

### 5.2 解压
```
tar -xf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz
```
将 phoenix-5.0.0-HBase-2.0-server.jar 和
phoenix-core-5.0.0-HBase-2.0.jar 拷贝到所有节点 Hbase/lib 下
```
cp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-5.0.0-HBase-2.0-server.jar /opt/pkg/hbase/lib/

cp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-core-5.0.0-HBase-2.0.jar /opt/pkg/hbase/lib/

scp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-core-5.0.0-HBase-2.0.jar node2:/opt/pkg/hbase/lib/

scp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-core-5.0.0-HBase-2.0.jar node3:/opt/pkg/hbase/lib/

scp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-core-5.0.0-HBase-2.0.jar node2:/opt/pkg/hbase/lib/

scp apache-phoenix-5.0.0-HBase-2.0-bin/phoenix-core-5.0.0-HBase-2.0.jar node3:/opt/pkg/hbase/lib/
```

下载 htrace-core-3.1.0-incubating.jar 依赖包到 hbase/lib 目录下
```
wget https://repo1.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar -O /opt/pkg/hbase/lib/htrace-core-3.1.0-incubating.jar
```

### 5.3 启动
```
重启 hbase
stop-hbase.sh
start-hbase.sh


cd apache-phoenix-5.0.0-HBase-2.0-bin/bin/
./sqlline.py node1:2181
连接 zookeeper 任一节点
```

### 5.4 下载 squirrel-sql-4.0.0-standard
详细配置请参考 https://blog.csdn.net/haiyang_duan/article/details/60594425
```
phoenix
jdbc:phoenix:172.*.*.27,172.*.*.28,172.*.*.29:2181
org.apache.phoenix.jdbc.PhoenixDriver
```

**注意：需要添加 hosts 映射，因为 zookeeper 存放的是HBase 节点的 hostname 而不是 IP**

## 6. Hive

节点 | IP
-- | --
node1 | 172.*.*.27

### 6.1 下载
```
wget http://mirrors.hust.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz -O /opt/pkg/apache-hive-3.1.2-bin.tar.gz
```

### 6.2 解压
```
cd /opt/pkg
tar -xf apache-hive-3.1.2-bin.tar.gz
ln -s apache-hive-3.1.2-bin hive
```

### 6.3 配置
#### （1）hive-env.sh
```
cp /opt/pkg/hive/conf/hive-env.sh.template /opt/pkg/hive/conf/hive-env.sh

vi /opt/pkg/hive/conf/hive-env.sh
```
```
添加
export HADOOP_HOME=/opt/pkg/hadoop
export HIVE_CONF_DIR=/opt/pkg/hive/conf
export HIVE_AUX_JARS_PATH=/opt/pkg/hive/lib
```

#### （2）hive-site.xml
```
cp /opt/pkg/hive/conf/hive-default.xml.template /opt/pkg/hive/conf/hive-site.xml

vi /opt/pkg/hive/conf/hive-site.xml
```
```
<configuration>

    <!-- mysql 配置，请修改<mysql-ip> -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://node3:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>


    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
    </property>
    <property>
        <name>datanucleus.readOnlyDatastore</name>
        <value>false</value>
    </property>
    <property>
        <name>datanucleus.fixedDatastore</name>
        <value>false</value>
    </property>
    <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.autoCreateTables</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.autoCreateColumns</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.metastore.local</name>
        <value>true</value>
    </property>
    <!-- 显示表的列名 -->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <!-- 显示数据库名称 -->
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>


    <!-- 配置 HBase 连接依赖 -->
    <property>
        <name>hive.aux.jars.path</name>
        <value>file:///opt/pkg/hbase/lib/hbase-client-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-common-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-protocol-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-server-2.2.3.jar,file:///opt/pkg/hbase/lib/htrace-core-3.1.0-incubating.jar,file:///opt/pkg/hbase/lib/zookeeper-3.4.10.jar</value>
    </property>

    <!-- 配置 HBase 使用的 Zookeeper -->
    <property>
        <name>hive.zookeeper.quorum</name>
        <value>node1,node2,node3</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>node1,node2,node3</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>

    <!-- kylin 在 load 表的时候，要调用 StorageSchemaReader 接口的 readSchema 方法，默认的 default 是没有这个实现方法的 -->
    <property>
        <name>metastore.storage.schema.reader.impl</name>
        <value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>
    </property>
</configuration>
```

#### （3）配置环境变量
```
vi /etc/profile
```
```
# hive
export HIVE_HOME=/opt/pkg/hive
export PATH=$HIVE_HOME/bin:$PATH
```
```
source /etc/profile
```

### 6.4 Mysql 依赖包
```
下载mysql-connector-java-8.0.17.jar 添加到/opt/pkg/hive/lib下

wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.17/mysql-connector-java-8.0.17.jar -O /opt/pkg/hive/lib/mysql-connector-java-8.0.17.jar
```
**请注意，低版本驱动会出错**

### 6.5 格式化数据库
```
schematool -dbType mysql -initSchema
```

### 6.6 异常处理
**异常 1**
```
NoSuchMethodError: com.google.common.base.Preconditions.checkArgument
```
原因:
```
hive 内依赖的 guava.jar 和 hadoop 内的版本不一致造成的。
```
检验方法:
- 查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本
- 查看hive安装目录下lib内guava.jar的版本 如果两者不一致

解决方法:
```
删除版本低的，并拷贝高版本
```


**异常 2**
```
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driver
Underlying cause: java.lang.ClassNotFoundException : com.mysql.jdbc.Driver
```
原因:
```
mysql 6.0 版本及之后版本，驱动改为 com.mysql.cj.jdbc.Driver
```

**异常 3**
```
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxUnexpectedCharException: Unexpected character '=' (code 61); expected a semi-colon after the reference for entity 'useUnicode'
 at [row,col,system-id]: [22,94,"file:/opt/pkg/apache-hive-3.1.2-bin/conf/hive-site.xml"]
```
原因:
```
xml 配置文件中不能直接使用 & 符号
```
解决方法:
```
使用 &amp; 代替 & 符号
```


### 6.7 连接客户端

#### （1）方式一
```
使用命令
hive
```

#### （2）方式二
```
nohup hiveserver2 1>/dev/null 2>&1 &
beeline -u jdbc:hive2://localhost:10000 -n root
```

### 6.8 验证
连接 hive
```
hive
```
创建 hbase 映射表
```
CREATE EXTERNAL TABLE TEST(
  TEST_ID string comment "ID",
  FIRST_NAME string comment "姓",
  LAST_NAME string comment "名",
  AGE string comment "年龄",
  GENDER string comment "性别"
) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,name:first,name:last,info:age,info:gender")
TBLPROPERTIES("hbase.table.name" = "test");
```
查询
```
SELECT * FROM TEST;

1       zhang   zhicheng        100     男
Time taken: 1.881 seconds, Fetched: 1 row(s)
```
删除表
```
drop table if exists TEST;
```
退出
```
exit;
```

### 6.9 常用命令
```
# 创建数据库
creat database if not exists database_name

# 查看所有数据库
show databases;
show databases like 'h.*';

# 使用哪个数据库
use default;

# 查看该数据库中的所有表
show tables;

# 支持模糊查询
show tables  ‘*t*’;

# 查看表的结构及表的路径
describe tab_name;    --

# 查看数据库的描述及路径
describe database database_name;
creat database database_name location '路径';

# 删除空的数据库
drop database if exists database_name;

# 先删除数据库中的表再删除数据库
drop database if exists database_name cascade;

# 查看表有哪些分区
show partitions t1;

# 删除表，删除 hive 的外部映射表，hbase 内的表不会被删除
drop table if exists tableName;
```





## 7. Spark
节点 | IP | 角色
-- | -- | --
node1 | 172.*.*.27 | master
node2 | 172.*.*.28 | master、worker
node3 | 172.*.*.29 | worker

### 7.1 下载
```
http://spark.apache.org/downloads.html

注意选择 Hadoop 版本
```

### 7.2 解压
```
tar -xf spark-3.0.0-preview2-bin-hadoop3.2.tgz -C /opt/pkg

cd /opt/pkg
ln -s spark-3.0.0-preview2-bin-hadoop3.2/ spark
```

### 7.3 配置
#### （1）spark-env.sh
```
cp /opt/pkg/spark/conf/spark-env.sh.template /opt/pkg/spark/conf/spark-env.sh

vi /opt/pkg/spark/conf/spark-env.sh
```
```
# 配置JAVA_HOME
export JAVA_HOME=/opt/pkg/jdk

# 配置HADOOP_HOME
export HADOOP_HOME=/opt/pkg/hadoop

# 配置HADOOP配置文件路径
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)

# 提交Application的端口，默认就是这个，万一要改呢，改这里
export SPARK_MASTER_PORT=7077

# 每一个Worker最多可以使用的cpu core的个数
# 真实服务器如果有32个，你可以设置为32个
export SPARK_WORKER_CORES=1

# 每一个Worker最多可以使用的内存，我的虚拟机就2g
# 真实服务器如果有128G，你可以设置为100G
export SPARK_WORKER_MEMORY=1g

# 启动spark historyService，18080为WEB端口，mycluster为HDFS的nameservices，不用带端口
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=2 -Dspark.history.fs.logDirectory=hdfs://mycluster/historyserverforSpark"

# 基于zookeeper的HA配置
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark"
```

#### （2）spark-defaults.conf
```
cp /opt/pkg/spark/conf/spark-defaults.conf.template /opt/pkg/spark/conf/spark-defaults.conf

vi /opt/pkg/spark/conf/spark-defaults.conf
```
```
spark.eventLog.enabled             true
spark.eventLog.compress            true
spark.eventLog.dir                 hdfs://mycluster/historyserverforSpark
spark.yarn.historyServer.address   mycluster:18080
spark.history.fs.logDirectory      hdfs://mycluster/historyserverforSpark
spark.speculation                  true
```

#### （3）slaves
```
cp /opt/pkg/spark/conf/slaves.template /opt/pkg/spark/conf/slaves

vi /opt/pkg/spark/conf/slaves
```
```
node2
node3
```


#### （4）修改启动 shell 文件名
```
修改启动脚本名称，以区分 spark 和 hadoop

mv /opt/pkg/spark/sbin/start-all.sh /opt/pkg/spark/sbin/start-spark.sh
mv /opt/pkg/spark/sbin/stop-all.sh /opt/pkg/spark/sbin/stop-spark.sh
```

#### （5）同步配置
```
在其他节点创建目录
mkdir /opt/pkg/spark
chown hadoop:hadoop -R /opt/pkg/spark

scp -r /opt/pkg/spark/* node2:/opt/pkg/spark/
scp -r /opt/pkg/spark/* node3:/opt/pkg/spark/
```


#### （6）环境变量
```
vi /etc/profile
```
```
# spark
export SPARK_HOME=/opt/pkg/spark
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
```
```
source /etc/profile
```


### 7.4 启动
```
在第一个 master 节点启动
start-spark.sh

在第二个 master 节点启动
start-master.sh
```

启动 history server
```
创建 Hadoop 存储目录
hdfs dfs -fs hdfs://node1:9000 -mkdir /historyserverforSpark
此处 -fs hdfs://node1:9000 参数是当前 active 状态 master 节点

start-history-server.sh
访问 http://node1:18080 查看 job history
```

### 7.5 验证
```
访问
http://node1:8080
http://node2:8080

查看 status
一个是 alive ，另一个是 standby

查看 alive 状态的页面 worker 节点数量是否正确
并且 state 是否为 alive
```

创建文件
```
vi test1.txt
```
内容为:
```
123
234
456
111
```
上传 Hadoop
```
hdfs dfs -fs hdfs://node1:9000 -mkdir /test
hdfs dfs -fs hdfs://node1:9000 -put test1.txt /test/
```

执行spark-shell
```
spark-shell
scala> val textCount = sc.textFile("/test/test1.txt").filter(line => line.contains("1")).count()

输出结果为
textCount: Long = 2

查看历史页面
http://node1:18080
可以看到执行的 spark-shell 的 job
```

** master 节点停机测试**
```
停止 alive 的 master 节点
stop-master.sh

等待一小段时间，可以看到
原本 standby 的 master 节点变更为 alive
worker 节点注册到次 master
```

## 8. Kylin

节点 | IP
node1 | 172.*.*.27

### 8.1 下载
```
https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-3.0.0/apache-kylin-3.0.0-bin-hadoop3.tar.gz
```

### 8.2 解压
```
tar -xf apache-kylin-3.0.0-bin-hadoop3.tar.gz -C /opt/pkg

cd /opt/pkg
ln -s apache-kylin-3.0.0-bin-hadoop3 kylin
```

### 8.3 配置

#### （1）配置环境变量
```
vi /etc/profile
```
```
# kylin
export KYLIN_HOME=/opt/pkg/kylin
export KYLIN_CONF=/opt/pkg/kylin/conf
export PATH=$PATH:$KYLIN_HOME/bin
```

#### （2）catalina.properties
vi /opt/pkg/kylin/tomcat/conf/catalina.properties
```
shared.loader=/opt/pkg/hive/lib/*.jar
```

#### （3）hbase
vi /opt/pkg/hbase/bin/hbase
```
找到
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
这一行

然后改成
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar:/opt/pkg/hbase/lib/*
```


### 8.4 问题
**问题 1**
```
hbase-common lib not found
```

解决方法:
```
vi /opt/pkg/hbase/bin/hbase

找到
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
这一行

然后改成
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar:/opt/pkg/hbase/lib/*
```

### 8.5 启动
```
kylin.sh start

访问 Web
http://node1:7070/kylin
用户名/密码 ADMIN/KYLIN
```

### 8.6 测试
```
运行
$KYLIN_HOME/bin/sample.sh

重启 kylin，然后 build cube
```


## 9. Grafana

节点 | IP
-- | --
node3 | 172.*.*.29

### 9.1 下载
```
wget https://dl.grafana.com/oss/release/grafana-6.5.1-1.x86_64.rpm
```

### 9.2 安装
```
yum localinstall grafana-6.5.1-1.x86_64.rpm
```

**安装插件**
```
grafana-cli plugins install blackmirror1-statusbygroup-panel
```

**日志**
```
/var/log/grafana/grafana.log
```

### 9.3 SMTP 配置
vi /etc/grafana/grafana.ini

```
[smtp]
enabled = true
host = smtp.qq.com:465
user = xxxx@qq.com
# If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
password = xxxxxx
;cert_file =
;key_file =
;skip_verify = false
from_address = xxxxxx@qq.com
from_name = Grafana
# EHLO identity in SMTP dialog (defaults to instance_name)
;ehlo_identity = dashboard.example.com

[emails]
welcome_email_on_sign_up = false
```
**注: password 是邮箱服务器提供的第三方授权码**

### 9.4 启动
```
systemctl start grafana-server
```

访问：
```
http://172.*.*.29:3000

用户/密码 admin/admin
```




### 9.5 异常
#### （1）登录之后出现 Unauthorized
```
更换浏览器
```


## 10. Web UI

hadoop
```
http://node1:50070
http://node2:50070
```

yarn
```
http://node1:8088/cluster/nodes
```

yarn jobhistory
```
http://172.*.*.27:19888/
```

hbase
```
http://node1:60010
```

spark jobHistory
```
http://node1:18080
http://node2:18080
```

Kylin
```
http://node1:7070/kylin
```

grafana
```
http://node3:3000
```

## 11. 集成 Kerberos 认证 （复杂）
节点 | IP | 角色
-- | -- | --
node1 | 172.*.*.27 | kerberos server

### 11.1 server 安装
```
yum -y install krb5-server krb5-libs krb5-workstation
```

### 11.2 server 配置
#### （1）kdc.conf
vi /var/kerberos/krb5kdc/kdc.conf
```
[kdcdefaults]
kdc_ports = 88
kdc_tcp_ports = 88
[realms]
HADOOP.COM = {
#master_key_type = aes256-cts
acl_file = /var/kerberos/krb5kdc/kadm5.acl
dict_file = /usr/share/dict/words
admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
max_renewable_life = 7d
supported_enctypes = aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
}
```

#### （2）kadm5.acl
vi /var/kerberos/krb5kdc/kadm5.acl
```
*/admin@HADOOP.COM      *
```

#### （3）krb5.conf
vi /etc/krb5.conf
```

[libdefaults]
  renew_lifetime = 7d
  forwardable = true
  default_realm = EXAMPLE.COM
  ticket_lifetime = 24h
  dns_lookup_realm = false
  dns_lookup_kdc = false
  default_ccache_name = /tmp/krb5cc_%{uid}
  #default_tgs_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5
  #default_tkt_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5

[logging]
  default = FILE:/var/log/krb5kdc.log
  admin_server = FILE:/var/log/kadmind.log
  kdc = FILE:/var/log/krb5kdc.log

[realms]
  EXAMPLE.COM = {
    admin_server = node5
    kdc = node5
  }

[domain_realm]
  .example.com = EXAMPLE.COM
  example.com = EXAMPLE.COM
```

#### （4）server 创建数据库
```
[root@node1 pkg]# kdb5_util create –r EXAMPLE.COM -s

Loading random data
Initializing database '/var/kerberos/krb5kdc/principal' for realm 'EXAMPLE.COM',
master key name 'K/M@EXAMPLE.COM'
You will be prompted for the database Master Password.
It is important that you NOT FORGET this password.
Enter KDC database master key:
Re-enter KDC database master key to verify:
```

#### （5）创建 Kerberos 的管理账号
```
[root@node1 pkg]# kadmin.local

Authenticating as principal hadoop/admin@EXAMPLE.COM with password.
kadmin.local:  addprinc admin/admin@EXAMPLE.COM
WARNING: no policy specified for admin/admin@EXAMPLE.COM; defaulting to no policy
Enter password for principal "admin/admin@EXAMPLE.COM":
Re-enter password for principal "admin/admin@EXAMPLE.COM":
Principal "admin/admin@EXAMPLE.COM" created.

本次 password 使用 abc123
```

#### （6）服务启动
```
systemctl start krb5kdc
systemctl start kadmin

自启动
systemctl enable krb5kdc
systemctl enable kadmin
```

#### （7）测试管理员帐号
```
[root@node1 ~]# kinit admin/admin@EXAMPLE.COM
Password for admin/admin@EXAMPLE.COM:

[root@node1 ~]# klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: admin/admin@EXAMPLE.COM

Valid starting       Expires              Service principal
01/18/2020 16:14:30  01/19/2020 16:14:30  krbtgt/EXAMPLE.COM@EXAMPLE.COM
```


### 11.3 zookeeper 配置 kerberos
#### （1）安装 client
```
所有 zookeeper 节点安装
yum install -y krb5-libs krb5-workstation

同步 server 配置，将 krb5.conf 配置拷贝到 zookeeper 节点
scp node1:/etc/krb5.conf /etc/
```

#### （2）kerberos server 创建 principal
```
[root@node1 ~]# kadmin.local
Authenticating as principal admin/admin@HADOOP.COM with password.
kadmin.local:  addprinc -randkey zookeeper/node1@HADOOP.COM

addprinc -randkey zookeeper/node2@HADOOP.COM

addprinc -randkey zookeeper/node3@HADOOP.COM

addprinc -randkey host/node1@HADOOP.COM

addprinc -randkey host/node2@HADOOP.COM

addprinc -randkey host/node3@HADOOP.COM

addprinc -randkey hadoop/node1@HADOOP.COM

addprinc -randkey hadoop/node2@HADOOP.COM

addprinc -randkey hadoop/node3@HADOOP.COM

addprinc -randkey HTTP/node1@HADOOP.COM

addprinc -randkey HTTP/node2@HADOOP.COM

addprinc -randkey HTTP/node3@HADOOP.COM

```

#### （3）kerberos server 生成 keytab 文件
```
kadmin.local:  xst -k zookeeper.keytab zookeeper/node1@HADOOP.COM

xst -k zookeeper.keytab zookeeper/node2@HADOOP.COM

xst -k zookeeper.keytab zookeeper/node3@HADOOP.COM

xst -k host.keytab host/node1@HADOOP.COM

xst -k host.keytab host/node2@HADOOP.COM

xst -k host.keytab host/node3@HADOOP.COM

xst -k hadoop.keytab hadoop/node1@HADOOP.COM

xst -k hadoop.keytab hadoop/node2@HADOOP.COM

xst -k hadoop.keytab hadoop/node3@HADOOP.COM

xst -k HTTP.keytab HTTP/node1@HADOOP.COM

xst -k HTTP.keytab HTTP/node2@HADOOP.COM

xst -k HTTP.keytab HTTP/node3@HADOOP.COM
```
```
生成的 keytab 文件会在当前目录下
使用 ktutil 合并
[hadoop@node1 hadoop]$ ktutil
ktutil:  rkt hadoop.keytab
ktutil:  rkt host.keytab
ktutil:  wkt hadoop.keytab
ktutil:  clear
ktutil:
ktutil:  rkt HTTP.keytab
ktutil:  rkt host.keytab
ktutil:  wkt HTTP.keytab
ktutil:  clear
```
```
chown hadoop:hadoop *.keytab
chmod 700 *.keytab

su hadoop

scp zookeeper.keytab node1:/opt/pkg/zookeeper/conf/
scp zookeeper.keytab node2:/opt/pkg/zookeeper/conf/
scp zookeeper.keytab node3:/opt/pkg/zookeeper/conf/

scp *.keytab node1:/opt/pkg/hadoop/etc/hadoop/
scp *.keytab node2:/opt/pkg/hadoop/etc/hadoop/
scp *.keytab node3:/opt/pkg/hadoop/etc/hadoop/
```

#### （4）zoo.cfg
vi /opt/pkg/zookeeper/conf/zoo.cfg
```
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/data/zookeeper
clientPort=2181

admin.serverPort=8888

server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888

# kerberos 配置
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
jaasLoginRenew=3600000
kerberos.removeHostFromPrincipal=true
kerberos.removeRealmFromPrincipal=true
```

#### （5）jaas.conf
所有节点修改
vi /opt/pkg/zookeeper/conf/jaas.conf
```
Server {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab="/opt/pkg/hadoop/etc/hadoop/hadoop.keytab"
  storeKey=true
  useTicketCache=false
  principal="hadoop/node1@HADOOP.COM";
};

注意，principal 每个 zookeeper 节点配置不一样，node1、node2、node3
```
#### （6）client-jaas.conf
vi /opt/pkg/zookeeper/conf/client-jaas.conf
```
Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab="/opt/pkg/zookeeper/conf/zookeeper.keytab"
  storeKey=true
  useTicketCache=false
  principal="zookeeper/node1@HADOOP.COM";
};

注意，principal 每个 zookeeper 节点配置不一样，node1、node2、node3
```

#### （7）java.env
vi /opt/pkg/zookeeper/conf/java.env
```
export JVMFLAGS="-Djava.security.auth.login.config=/opt/pkg/zookeeper/conf/jaas.conf"
export CLIENT_JVMFLAGS="-Djava.security.auth.login.config=/opt/pkg/zookeeper/conf/client-jaas.conf"
```

#### （8）同步配置
```
scp /opt/pkg/zookeeper/conf/* node2:/opt/pkg/zookeeper/conf/
scp /opt/pkg/zookeeper/conf/* node3:/opt/pkg/zookeeper/conf/

同步之后，请修改 jaas.conf 和 client-jaas.conf 的 principal
```

#### （9）异常
java.io.IOException: Could not configure server because SASL configuration did not allow the  ZooKeeper server to authenticate itself properly: javax.security.auth.login.LoginException: Message stream modified
```
kerberos 配置有错误，直接用原本的配置也行
```

### 11.4 hadoop 配置 kerberos

#### （1）修改 core-site.xml
vi /opt/pkg/hadoop/etc/hadoop/core-site.xml
```
<configuration>
        <!-- 指定hdfs的nameservice为node，需要和hdfs-site.xml中的dfs.nameservices一致 -->
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://mycluster</value>
        </property>

        <!--指定hadoop数据临时存放目录-->
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/data/hadoop/tmp</value>
        </property>

        <!--指定hdfs操作数据的缓冲区大小 可以不配-->
        <property>
                <name>io.file.buffer.size</name>
                <value>131072</value>
        </property>

        <!--指定zookeeper地址-->
        <property>
                <name>ha.zookeeper.quorum</name>
                <value>node1:2181,node2:2181,node3:2181</value>
        </property>

        <!-- kerberos 认证配置 -->
        <property>
                <name>hadoop.security.authentication</name>
                <value>kerberos</value>
        </property>

        <property>
                <name>hadoop.security.authorization</name>
                <value>true</value>
        </property>

        <property>
                <name>hadoop.rpc.protection</name>
                <value>authentication</value>
        </property>

        <property>
                <name>hadoop.security.auth_to_local</name>
                <value>DEFAULT</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hive.hosts</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hive.groups</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hdfs.hosts</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hdfs.groups</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.HTTP.hosts</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.HTTP.groups</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hadoop.hosts</name>
                <value>*</value>
        </property>

        <property>
                <name>hadoop.proxyuser.hadoop.groups</name>
                <value>*</value>
        </property>
</configuration>
```

#### （2）修改 hdfs-site.xml
vi /opt/pkg/hadoop/etc/hadoop/hdfs-site.xml
```
<configuration>
        <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的fs.defaultFS对应 -->
        <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
        </property>

        <!-- mycluster下面有两个NameNode，分别是namenode1，namenode2, 名字可自定义, 注意与通信地址配置一致即可 -->
        <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>namenode1,namenode2</value>
        </property>

        <!-- namenode1的RPC通信地址, 注意name最后两个名称 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode1</name>
                <value>node1:9000</value>
        </property>

        <!-- namenode1的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode1</name>
                <value>node1:50070</value>
        </property>

        <!-- namenode2的RPC通信地址 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode2</name>
                <value>node2:9000</value>
        </property>
        <!-- namenode2的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode2</name>
                <value>node2:50070</value>
        </property>

        <!-- 设置一组 journalNode 的 URI 地址，active NameNode 将 edit log 写入这些JournalNode -->
        <!-- 而 standby NameNode 读取这些 edit log，并作用在内存中的目录树中。-->
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://node1:8485;node2:8485;node3:8485/mycluster</value>
        </property>

        <!-- JournalNode 存储路径 -->
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/data/hadoop/journal/</value>
        </property>

        <!-- 数据块大小，默认为64M -->
        <property>
                <name>dfs.block.size</name>
                <value>134217728</value>
        </property>

        <!-- 对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。-->
        <!-- 设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。-->
        <!-- 疑问: 集群大小是如何定义的?-->
        <property>
                <name>dfs.namenode.handler.count</name>
                <value>10</value>
        </property>

        <!-- DataNode 存储路径 -->
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/data/hadoop/data/</value>
        </property>

        <!-- 块副本数 -->
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>

        <!-- 关闭权限管理 -->
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>

        <!-- NameNode 存储路径 -->
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/data/hadoop/name/</value>
        </property>

        <!-- 开启NameNode故障时自动切换 -->
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>

        <!-- 指定mycluster（与最上方dfs.nameservices配置对应）出故障时，哪个实现类负责执行故障切换，注意：mycluster一定要与nameservices配置一致 -->
        <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>

        <!-- 发生故障时，避免两个NameNode都为Active状态，使用ssh方式kill掉一个 -->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>
                    sshfence
                    shell(/bin/true)
                </value>
        </property>

        <!-- 配置sshfence使用的私钥路径 -->
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/home/hadoop/.ssh/id_rsa</value>
        </property>

        <!-- 配置sshfence超时时长 -->
        <property>
                <name>dfs.ha.fencing.ssh.connect-timeout</name>
                <value>5000</value>
        </property>

        <!-- 使用IP配置 -->
        <property>
                <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
                <value>false</value>
        </property>



        <!-- kerberos 认证配置 -->
        <property>
                <name>dfs.block.access.token.enable</name>
                <value>true</value>
        </property>

        <property>
                <name>dfs.web.authentication.kerberos.principal</name>
                <value>HTTP/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.web.authentication.kerberos.keytab</name>
                <value>/opt/pkg/hadoop/etc/hadoop/HTTP.keytab</value>
        </property>

        <property>
                <name>dfs.namenode.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>dfs.namenode.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.datanode.address</name>
                <value>0.0.0.0:1004</value>
        </property>

        <property>
                <name>dfs.datanode.http.address</name>
                <value>0.0.0.0:1006</value>
        </property>

        <property>
                <name>dfs.secondary.namenode.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>dfs.secondary.namenode.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.namenode.secondary.kerberos.internal.spnego.principal</name>
                <value>HTTP/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.datanode.data.dir.perm</name>
                <value>700</value>
        </property>

        <property>
                <name>dfs.datanode.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>dfs.datanode.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.datanode.kerberos.https.principal</name>
                <value>HTTP/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.journalnode.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>dfs.journalnode.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
                <value>HTTP/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>dfs.data.transfer.protection</name>
                <value>integrity</value>
        </property>

        <property>
                <name>dfs.encrypt.data.transfer</name>
                <value>true</value>
        </property>
</configuration>
```

#### （3）修改 hadoop-env.sh
vi /opt/pkg/hadoop/etc/hadoop/hadoop-env.sh
```
# 配置JAVA_HOME
export JAVA_HOME=/opt/pkg/jdk

export HDFS_DATANODE_SECURE_USER=hadoop
export HADOOP_SECURE_PID_DIR=$HADOOP_HOME/pids
export HADOOP_SECURE_LOG_DIR=$HADOOP_HOME/logs
export JSVC_HOME=/usr/bin
```

#### （4）修改 yarn-site.xml
vi /opt/pkg/hadoop/etc/hadoop/yarn-site.xml
```
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>node1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>node2</value>
    </property>
    <!--开启故障自动切换-->
    <property>
       <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
       <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>node1:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>node2:8088</value>
    </property>

    <!--配置与zookeeper的连接地址-->
    <property>
      <name>yarn.resourcemanager.zk-state-store.address</name>
      <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>

    <!--开启自动恢复功能-->
    <property>
      <name>yarn.resourcemanager.recovery.enabled</name>
      <value>true</value>
    </property>


    <property>
      <name>yarn.nodemanager.pmem-check-enabled</name>
      <value>false</value>
    </property>
    <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
    </property>
    <!-- 指定nodemanager启动时加载server的方式为shuffle server -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>


    <!-- kerberos 认证配置 -->
    <property>
        <name>yarn.resourcemanager.keytab</name>
        <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
    </property>
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hadoop/_HOST@HADOOP.COM</value>
    </property>
    <property>
        <name>yarn.nodemanager.keytab</name>
        <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
    </property>
    <property>
        <name>yarn.nodemanager.principal</name>
        <value>hadoop/_HOST@HADOOP.COM</value>
    </property>
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>hadoop</value>
    </property>

</configuration>
```

#### （5）修改 mapred-site.xml
vi /opt/pkg/hadoop/etc/hadoop/mapred-site.xml
```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node1:10020</value>
    </property>

    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node1:19888</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/history/done</value>
    </property>
    <property>
        <name>mapreudce.jobhistory.intermediate.done-dir</name>
        <value>/history/done/done_intermediate</value>
    </property>


    <!-- kerberos 认证配置 -->
    <property>
        <name>mapreduce.jobhistory.keytab</name>
        <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.principal</name>
        <value>hadoop/_HOST@HADOOP.COM</value>
    </property>
</configuration>
```

#### （6）container-executor
```
hadoop 的 container-executor 是32位，且要用 root 用户执行，父级目录也需要是 root 用户的

下载同版本的 hadoop source code
http://apache.communilink.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.1-src.tar.gz

tar -xf hadoop-3.2.1-src.tar.gz
cd hadoop-3.2.1-src

yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake
yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool

编译
    mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true -Dcontainer-executor.conf.dir=/etc

将 ./hadoop-dist/target/hadoop-3.2.1/bin/container-executor 拷贝到 $HADOOP_HOME/bin
将 $HADOOP_HOME/etc/hadoop/container-executor.cfg 拷贝到 /etc/ 下

授权
    chown root:hadoop container-executor /etc/container-executor.cfg
    chmod 644 /etc/container-executor.cfg
    chmod 6050 container-executor

修改 /etc/container-executor.cfg
    yarn.nodemanager.linux-container-executor.group=hadoop
    banned.users=hadoop
```

#### （7）安装 jsvc
所有节点安装 jsvc
```
yum install -y jsvc
```

#### （8）更换 commons-daemon jar 包
```
mv /opt/pkg/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar /opt/pkg/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar.bak

wget https://repo1.maven.org/maven2/commons-daemon/commons-daemon/1.0.15/commons-daemon-1.0.15.jar -O /opt/pkg/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.15.jar
```

#### （9）启动
```
关闭 kylin、spark、hbase
    kylin.sh stop
    stop-hbase.sh
    stop-spark.sh
    stop-history-server.sh

在 master-standby（node2）关闭 master
    stop-master.sh

关闭 hadoop
    stop-all.sh
    mr-jobhistory-daemon.sh stop historyserver  或者  mapred --daemon stop historyserver

重启 zookeeper
systemctl restart zookeeper

启动 hadoop
    start-all.sh
    mapred --daemon start historyserver

切换至 root 用户，启动 datanode
start-secure-dns.sh
注：带有安全的 datanode 必须以 root 启动
```


#### （10）验证
```
直接执行
    hdfs dfs -fs hdfs://node1:9000 -ls /
会出现
    ls: failure to login: javax.security.auth.login.LoginException: java.lang.IllegalArgumentException: Illegal principal name HTTP/**@EXAMPLE.COM: org.apache.hadoop.security.

登录 （*号替换为当前机器名）
    kinit -kt /opt/pkg/hadoop/etc/hadoop/hadoop.keytab HTTP/**@EXAMPLE.COM
然后执行
    hdfs dfs -fs hdfs://node1:9000 -ls /
```

#### （11）问题
```
问题:
    org.apache.hadoop.hdfs.qjournal.protocol.JournalNotFormattedException: Journal Storage Directory root= /data/hadoop/journal/mycluster; location= null not formatted ; journal id: mycluster

解决方法
    拷贝其他机器上的 journalnode 数据到此机器
```
```
问题：
    org.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 0.  Expected transaction ID was 1
```
```
编译问题：
    org.apache.maven.plugin.MojoExecutionException: 'protoc --version' did not return a version

解决方法：
    下载 protobuf 编译安装
    https://github.com/protocolbuffers/protobuf/releases/tag/v2.5.0
    tar -xf protobuf-2.5.0.tar.gz
    cd protobuf-2.5.0
    yum install -y cmake zlib* libssl*
    ./configure
    make && make install
```
```
编译问题：
    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.2.1:cmake-compile (cmake-compile) on project hadoop-common: CMake failed with error code 1

解决方法：
    yum install -y zlib* libssl*
    source ~/.bash_profile

    git clone https://github.com/Kitware/CMake.git
    cd CMake
    ./bootstrap
    make -j8
    make install
    make clean
    ln -s /usr/local/bin/cmake /usr/bin/cmake
```
```
编译问题：
    [ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.1:exec (pre-dist) on project hadoop-hdfs: Command execution failed.: Stream closed -> [Help 1]

解决方法：
    重新编译一次
```
```
编译问题：
    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.2.1:cmake-compile (cmake-compile) on project hadoop-hdfs-native-client: CMake failed with error code 1

解决方法：
    查看 hadoop-hdfs-project/hadoop-hdfs-native-client/target/CMakeFiles/ 目录下的 CMakeError.log 错误日志，然后解决
```
```
编译问题：
    从 CMakeError.log 日志可以看到 /usr/bin/ld: cannot find -lpthreads 错误
```

### 11.5 hbase 配置 kerberos
#### （1）修改 hbase-site.xml
vi /opt/pkg/hbase/conf/hbase-site.xml
```
<configuration>
        <!-- 指定hbase在HDFS上存储的路径，注意需要与hadoop的配置相对于 -->
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://mycluster/data/hbase</value>
        </property>

        <!-- 指定hbase是分布式的 -->
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>

        <!-- master节点 --> <!-- 个人理解，多master节点时，应该是不需要配置此项亦可，通过zookeeper实现服务发现 -->
        <!--
        <property>
                <name>hbase.master.hostname</name>
                <value>172.16.234.27</value>
        </property>
        -->

        <!-- 指定zookeeper的地址，多个用“,”分割 -->
        <property>
                <name>hbase.zookeeper.quorum</name>
                <value>node1,node2,node3</value>
        </property>

        <!-- 指定zookeeper端口 -->
        <property>
                <name>hbase.zookeeper.property.clientPort</name>
                <value>2181</value>
        </property>

        <!-- Master绑定的端口，包括backup-master -->
        <property>
                <name>hbase.master.port</name>
                <value>16000</value>
        </property>

        <!-- zookeeper存储路径 -->
        <property>
                <name>hbase.zookeeper.property.dataDir</name>
                <value>/data/zookeeper</value>
        </property>

        <!-- 临时文件存储路径 -->
        <property>
                <name>hbase.tmp.dir</name>
                <value>/data/hbase/tmp</value>
        </property>

        <property>
                <name>zookeeper.znode.parent</name>
                <value>/hbase</value>
        </property>

        <property>
                <name>hbase.unsafe.stream.capability.enforce</name>
                <value>false</value>
        </property>

        <property>
                <name>hbase.master.info.port</name>
                <value>60010</value>
        </property>

        <!-- HRegionServer 频繁宕掉时配置 -->
        <property>
                <name>hbase.coprocessor.abortonerror</name>
                <value>false</value>
        </property>


        <!-- kerberos 认证配置 -->
        <property>
                <name>hbase.security.authentication</name>
                <value>kerberos</value>
        </property>

        <property>
                <name>hbase.rpc.engine</name>
                <value>org.apache.hadoop.hbase.ipc.SecureRpcEngine</value>
        </property>

        <property>
                <name>hbase.regionserver.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>hbase.regionserver.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>hbase.master.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>hbase.master.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>hbase.coprocessor.region.classes</name>
                <value>org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController</value>
        </property>
</configuration>
```

#### （2）修改 hbase-env.sh
vi /opt/pkg/hbase/conf/hbase-env.sh
```
# JDK
export JAVA_HOME=/opt/pkg/jdk

# hbase配置文件的路径
export HBASE_CLASSPATH=/opt/pkg/hbase/conf

# 此配置信息，设置由独立的zk集群管理，故为false
export HBASE_MANAGES_ZK=false

# Hbase日志目录
export HBASE_LOG_DIR=/opt/pkg/hbase/logs

# 配置 kerberos 认证，指向 zookeeper 的 Client
export HBASE_OPTS="$HBASE_OPTS -Djava.security.auth.login.config=/opt/pkg/zookeeper/conf/jaas.conf"
```

#### （3）同步配置
```
scp /opt/pkg/hbase/conf/* node2:/opt/pkg/hbase/conf/
scp /opt/pkg/hbase/conf/* node3:/opt/pkg/hbase/conf/
```

#### （4）启动
```
start-hbase.sh
```

#### （5）验证
```
使用 hbase shell 验证

未登录的机器执行
[root@node2 logs]# hbase shell
Took 0.0038 seconds
hbase(main):001:0> list
TABLE
ERROR: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)

已登录的机器执行
[root@node2 logs]# hbase shell
Took 0.0038 seconds
hbase(main):001:0> list
TABLE
0 row(s)
Took 1.1678 seconds
=> []
```

#### （6）错误
Caused by: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hbase
```
zookeeper 的 jaas.conf、client-jaas.conf 和 java.env 配置错误
```

### 11.6 Phoenix 连接带有安全认证的 HBase
```
连接 URL
jdbc:phoenix:<zookeeper>:<port>:<principal>:<keytab>

e.g
jdbc:phoenix:172.*.*.83,172.*.*.84,172.*.*.85:2181:hadoop/node1@HADOOP.COM:‪D:\hadoop.keytab



sqlline.py
./sqlline.py <zookeeper>:<port>:<principal>:<keytab>

e.g
./sqlline.py node1,node2,node3:2181:hadoop/node1@HADOOP.COM:‪D:\hadoop.keytab
```

### 11.7 Hive 配置 Kerberos
#### （1）hive-site.xml
vi /opt/pkg/hive/conf/hive-site.xml
```
<configuration>
        <!-- mysql 配置，请修改<mysql-ip> -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://172.16.234.29:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>


    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
    </property>
    <property>
        <name>datanucleus.readOnlyDatastore</name>
        <value>false</value>
    </property>
    <property>
        <name>datanucleus.fixedDatastore</name>
        <value>false</value>
    </property>
    <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.autoCreateTables</name>
        <value>true</value>
    </property>
    <property>
        <name>datanucleus.autoCreateColumns</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.metastore.local</name>
        <value>true</value>
    </property>
    <!-- 显示表的列名 -->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <!-- 显示数据库名称 -->
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>


    <!-- 配置 HBase 连接依赖 -->
    <property>
        <name>hive.aux.jars.path</name>
        <value>file:///opt/pkg/hbase/lib/hbase-client-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-common-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-protocol-2.2.3.jar,file:///opt/pkg/hbase/lib/hbase-server-2.2.3.jar,file:///opt/pkg/hbase/lib/htrace-core-3.1.0-incubating.jar,file:///opt/pkg/hbase/lib/zookeeper-3.4.10.jar</value>
    </property>

    <!-- 配置 HBase 使用的 Zookeeper -->
    <property>
        <name>hive.zookeeper.quorum</name>
        <value>172.16.234.27,172.16.234.28,172.16.234.29</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>172.16.234.27,172.16.234.28,172.16.234.29</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>

    <!-- kylin 在 load 表的时候，要调用 StorageSchemaReader 接口的 readSchema 方法，默认的 default 是没有这个实现方法的 -->
    <property>
        <name>metastore.storage.schema.reader.impl</name>
        <value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>
    </property>



        <!-- kerberos 认证配置 -->
        <property>
                <name>hbase.coprocessor.region.classes</name>
                <value>org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController</value>
        </property>

        <property>
                <name>hive.server2.authentication</name>
                <value>KERBEROS</value>
        </property>

        <property>
                <name>hive.server2.authentication.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>

        <property>
                <name>hive.server2.authentication.kerberos.keytab</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>hive.metastore.sasl.enabled</name>
                <value>true</value>
        </property>

        <property>
                <name>hive.metastore.kerberos.keytab.file</name>
                <value>/opt/pkg/hadoop/etc/hadoop/hadoop.keytab</value>
        </property>

        <property>
                <name>hive.metastore.kerberos.principal</name>
                <value>hadoop/_HOST@HADOOP.COM</value>
        </property>
</configuration>
```

#### （2）hive-env.sh
vi /opt/pkg/hive/conf/hive-env.sh
```
# HBase kerberos
export HADOOP_HOME=/opt/pkg/hadoop

export HIVE_CONF_DIR=/opt/pkg/hive/conf

export HIVE_AUX_JARS_PATH=/opt/pkg/hive/lib

# HBase kerberos
export HIVE_OPTS="-hiveconf hbase.security.authentication=kerberos -hiveconf hbase.rpc.engine=org.apache.hadoop.hbase.ipc.SecureRpcEngine -hiveconf hbase.master.kerberos.principal=hadoop/_HOST@HADOOP.COM -hiveconf hbase.regionserver.kerberos.principal=hadoop/_HOST@HADOOP.COM -hiveconf hbase.zookeeper.quorum=node1,node2,node3"
```

#### （3）启动
```
# nohup hive --service metastore &
nohup hive --service hiveserver2 &
开启 debug
nohup hive -hiveconf hive.root.logger=DEBUG,console --service hiveserver2 &

使用 beeline 连接
[root@node1 bin]# beeline
beeline> !connect jdbc:hive2://node1:10000/default;principal=hadoop/node1@HADOOP.COM
```

#### （4）问题
```
问题：
    beeline 连接 hive 查询时会卡住，原因不明，解决方法不明
```
```
问题：
    FAILED: SemanticException Error while configuring input job properties

解决方法：
    在 hbase-site.xml 和 hive-site.xml 加上下面配置，并重启
    <property>
            <name>hbase.coprocessor.region.classes</name>
            <value>org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController</value>
    </property>
```


### 11.8 Spark
```
执行命令
    spark-shell

会出现报错
    2020-02-12 15:23:19,759 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]


执行
    kinit -kt /opt/pkg/hadoop/etc/hadoop/hadoop.keytab hadoop/node1@HADOOP.COM

再次执行
    spark-shell
    就不会报错了
```

## 12. 扩展
### 12.1 hive on tez （未实践，仅记录）
#### （1）下载
```
https://mirror.bit.edu.cn/apache/tez/

tar zxvf apache-tez-0.9.0-bin.tar.gz
mv apache-tez-0.9.0-bin tez

hdfs dfs -mkdir /tez-0.9.0
hdfs dfs -put tez/share/tez.tar.gz /tez-0.9.0
```

#### （2）配置
vi /opt/pkg/hadoop/etc/hadoop/tez-site.xml
```
<configuration>
    <property>
        <name>tez.lib.uris</name>
        <value>hdfs://mycluster/tez-0.9.0/tez.tar.gz</value>
    </property>

    <!-- tex ui -->
    <property>
        <name>tez.history.logging.service.class</name>
        <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
    </property>

    <property>
        <name>tez.tez-ui.history-url.base</name>
        <value>http://<IP>:9999/tez-ui/</value>
    </property>

    <property>
        <name>tez.allow.disabled.timeline-domains</name>
        <value>true</value>
    </property>

    <property>
        <name>tez.runtime.convert.user-payload.to.history-text</name>
        <value>true</value>
    </property>

    <property>
        <name>tez.task.generate.counters.per.io</name>
        <value>true</value>
    </property>
</configuration>
```

vi /opt/pkg/hadoop/etc/hadoop/yarn-site.xml
```
    <!-- timeline 配置 -->
    <property>
        <name>yarn.timeline-service.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.timeline-service.hostname</name>
        <value><tez-ui host></value>
    </property>

    <property>
        <name>yarn.timeline-service.http-cross-origin.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.timeline-service.generic-application-history.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.timeline-service.address</name>
        <value>${yarn.timeline-service.hostname}:10200</value>
    </property>

    <property>
        <name>yarn.timeline-service.webapp.address</name>
        <value>${yarn.timeline-service.hostname}:8188</value>
    </property>

    <property>
        <name>yarn.timeline-service.webapp.https.address</name>
        <value>${yarn.timeline-service.hostname}:8190</value>
    </property>

    <property>
        <name>yarn.timeline-service.handler-thread-count</name>
        <value>10</value>
    </property>

    <property>
        <name>yarn.timeline-service.generic-application-history.enabled</name>
        <value>false</value>
    </property>

    <property>
        <name>yarn.timeline-service.generic-application-history.store-class</name>
        <value>org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore</value>
    </property>
```

vi /opt/pkg/hadoop/etc/hadoop/mapred-site.xml
```
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn-tez</value>
    </property>
```

vi /opt/pkg/hadoop/etc/hadoop/hadoop-env.sh
```
export TEZ_HOME=/opt/pkg/tez
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/*:$TEZ_HOME/lib/*
```

vi /opt/pkg/hive/conf/hive-site.xml
```
    <property>
        <name>hive.user.install.directory</name>
        <value>/user/</value>
    </property>
    <property>
        <name>hive.execution.engine</name>
        <value>tez</value>
    </property>
```

#### （3）tez ui
将 /opt/pkg/tez/share/tez.tar.gz 提取 tez-ui-<version>.war 放到 tomcat 下

修改配置 config/configs.env 配置，注意与 yarn-site.xml 相对应
```
timeline: "http://<webapp-ip>:8188"

rm: "http://<webapp-ip>:8088"
```
