---
title: 双 master Hadoop 集群搭建
tags:
  - 大数据
  - hadoop
date: 2020-05-07 11:34:27
permalink: /pages/7a645a/
categories:
  - 大数据
  - hadoop
---

## 1. 下载地址
```
https://hadoop.apache.org/releases.html
```

## 2. 组件介绍
组件|功能
:--:|:--:
NameNode | 指导Slave端的DataNode执行I/O任务，他跟踪文件如何分割成文件块，然后被什么节点存储，分布式文件系统的运行状态是否正常。
DataNode | 每个Slave节点都会有个DataNode守护进程来执行文件系统的工作——将HDFS数据块读取或者写入本地系统的实际文件中。
SecondaryNameNode | 备份系统，备份NameNode的数据
DFSZKFailoverController | NameNode HA实现的中心组件，它负责整体的故障转移控制等。
ResourceManager | 当应用程序对集群资源需求时，ResourceManager是Yarn集群主控节点，负责协调和管理整个集群（所有NodeManager）的资源。
NodeManager | 管理Hadoop集群中单个计算节点，功能包含与ResourceManager保持通信，管理Container的生命周期、监控每一个Container的资源使用(内存、CPU等）情况、追踪节点健康状况、管理日志和不同应用程序用到的附属服务等。
JournalNode | NameNode通过其进行相互通信，active NameNode 将 edit log 写入这些JournalNode，而 standby NameNode 读取这些 edit log，并作用在内存中的目录树中。

<!-- more -->

## 3. 解压
```
tar -xf hadoop-<version>.tar.gz
```

## 4. 配置环境变量
```
# hadoop
export HADOOP_HOME=/路径/hadoop-3.1.2
export PATH=$HADOOP_HOME/bin:$PATH
export PATH=$HADOOP_HOME/sbin:$PATH
```

## 5. 配置
配置文件路径:
```
hadoop-<version>/etc/hadoop
```

<b>hdfs-site.xml 配置</b>
```
<configuration>

        <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的fs.defaultFS对应 -->
        <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
        </property>

        <!-- mycluster下面有两个NameNode，分别是nn1，nn2, 名字可自定义, 注意与通信地址配置一致即可 -->
        <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>namenode1,namenode2</value>
        </property>

        <!-- namenode1的RPC通信地址, 注意name最后两个名称 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode1</name>
                <value>namenode1:9000</value>
        </property>

        <!-- namenode1的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode1</name>
                <value>namenode1:50070</value>
        </property>

        <!-- namenode2的RPC通信地址 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.namenode2</name>
                <value>namenode2:9000</value>
        </property>
        <!-- namenode2的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.namenode2</name>
                <value>namenode2:50070</value>
        </property>

        <!-- 设置一组 journalNode 的 URI 地址，active NameNode 将 edit log 写入这些JournalNode -->
        <!-- 而 standby NameNode 读取这些 edit log，并作用在内存中的目录树中。-->
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://node1:8485;node2:8485/mycluster</value>
        </property>

        <!-- JournalNode 存储路径 -->
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/data/hadoop/journal/</value>
        </property>

        <!-- 数据块大小，默认为64M -->
        <property>
                <name>dfs.block.size</name>
                <value>134217728</value>
        </property>

        <!-- 对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。-->
        <!-- 设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。-->
        <!-- 集群大小是如何定义的，是datanode节点数量吗？？？-->
        <property>
                <name>dfs.namenode.handler.count</name>
                <value>20</value>
        </property>

        <!-- DataNode 存储路径 -->
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/data/hadoop/data/</value>
        </property>

        <!-- 块副本数 -->
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>

        <!-- 关闭权限管理 -->
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>

        <!-- NameNode 存储路径 -->
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/data/hadoop/name/</value>
        </property>

        <!-- 开启NameNode故障时自动切换 -->
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>

        <!-- 指定mycluster（与最上方dfs.nameservices配置对应）出故障时，哪个实现类负责执行故障切换，注意：mycluster一定要与nameservices配置一致 -->
        <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>

        <!-- 发生故障时，避免两个NameNode都为Active状态，使用ssh方式kill掉一个 -->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>
                    sshfence
                    shell(/bin/true)
                </value>
        </property>

        <!-- 配置sshfence使用的私钥路径 -->
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/root/.ssh/id_rsa</value>
        </property>

        <!-- 配置sshfence超时时长 -->
        <property>
                <name>dfs.ha.fencing.ssh.connect-timeout</name>
                <value>5000</value>
        </property>

</configuration>
```

<b>core-site.xml 配置</b>
```
<configuration>

  <!-- 指定hdfs的nameservice为node，需要和hdfs-site.xml中的dfs.nameservices一致 -->
  <property>
      <name>fs.defaultFS</name>
      <value>hdfs://mycluster</value>
  </property>

  <!--指定hadoop数据临时存放目录-->
  <property>
      <name>hadoop.tmp.dir</name>
      <value>/data/hadoop/tmp</value>
  </property>

  <!--指定hdfs操作数据的缓冲区大小 可以不配-->
  <property>
      <name>io.file.buffer.size</name>
      <value>131072</value>
  </property>

  <!--指定zookeeper地址-->
  <property>
      <name>ha.zookeeper.quorum</name>
      <value>zoo-node1:2181,zoo-node2:2181,zoo-node3:2181</value>
  </property>

</configuration>
```

<b>yarn-site.xml 配置</b>
```
<configuration>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>namenode1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>namenode2</value>
    </property>
    <!--开启故障自动切换-->
    <property>
       <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
       <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>namenode1:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>namenode2:8088</value>
    </property>

    <!--配置与zookeeper的连接地址-->
    <property>
      <name>yarn.resourcemanager.zk-state-store.address</name>
      <value>zoo-node1:2181,zoo-node2:2181,zoo-node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>zoo-node1:2181,zoo-node2:2181,zoo-node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>

    <!--开启自动恢复功能-->
    <property>
      <name>yarn.resourcemanager.recovery.enabled</name>
      <value>true</value>
    </property>

    <!-- 指定nodemanager启动时加载server的方式为shuffle server -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
</configuration>
```

<b>hadoop-env.sh</b>
```
# 配置JAVA_HOME
export JAVA_HOME=/路径/jdk
```

<b>start-dfs.sh、stop-dfs.sh</b>
```
cd ../../sbin
添加：
    HDFS_DATANODE_USER=root
    HDFS_DATANODE_SECURE_USER=hdfs
    HDFS_NAMENODE_USER=root
    HDFS_SECONDARYNAMENODE_USER=root
    HDFS_JOURNALNODE_USER=root
    HDFS_ZKFC_USER=root
```

<b>start-yarn.sh、stop-yarn.sh</b>
```
添加：
    YARN_RESOURCEMANAGER_USER=root
    HADOOP_SECURE_DN_USER=yarn
    YARN_NODEMANAGER_USER=root
```

<b>workers</b>
配置datanode节点
```
datanode1
datanode2
datanode3
```

同步所有配置
```
scp conf/* namenode2:`pwd`/conf/
scp conf/* datanode1:`pwd`/conf/
scp conf/* datanode2:`pwd`/conf/
scp conf/* datanode3:`pwd`/conf/
```

## 4. 启动
### 4.1 创建目录
namenode节点
```
mkdir -p /data/hadoop/name
mkdir /data/hadoop/journal
```
datanode节点
```
mkdir /data/hadoop/data
```

### 4.2 启动JournalNode
```
hadoop-daemon.sh start journalnode
```

### 4.3 格式化zkfc
```
node1机器执行
    hdfs zkfc -formatZK
```

### 4.4 格式化hdfs
```
node1机器执行
    hdfs namenode -format
```

### 4.5 同步数据
```
scp -r /data/hadoop namenode2:/data/

scp -r datanode1:/data/hadoop datanode2:/data/
scp -r datanode1:/data/hadoop datanode3:/data/
```

### 4.6 启动全部
```
./sbin/start-all.sh
```

## 5. 查看运行情况
```
使用 jps 命令查看各个组件是否在运行
namenode节点：
    NameNode
    NodeManager
    QuorumPeerMain
    DFSZKFailoverController
    ResourceManager
    JournalNode

datanode节点：
    JournalNode
    NodeManager
    DataNode
    QuorumPeerMain


访问
    http://namenode1:50070
    http://namenode2:50070
    查看NameNode是否一个为active，另一个为standby

    在页面上选择上方DataNodes查看datanode节点是否齐全

使用简单的hdfs命令
    hdfs dfs -fs hdfs://node1:9000 -ls /
    hdfs dfs -fs hdfs://node1:9000 -mkdir /test

    注意node1要替换为active状态的NameNode，不能用standby状态的

上传文件
    hdfs dfs -fs hdfs://node1:9000 -put test1.txt /test
```
