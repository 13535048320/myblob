---
title: Kubernetes 使用总结
tags:
  - docker
  - kubernetes
  - k8s
sticky: 1
date: 2020-08-05 10:20:46
permalink: /pages/d82f39/
categories:
  - 容器
  - kubernetes
---
# 架构图
![image](https://www.zzcdev.cn/myblob/mdimg/clipboard.png)

# 目录
- 1. Kubernetes 简介
- 2. 组件
- 3. 资源
- 4. 集群搭建（RKE）
- 5. 安装 Rancher
- 6. kubectl 集群管理命令
<!-- more -->

## 1. Kubernetes 简介
**Kubernetes 是当今最流行的开源容器管理平台，K8S是一个简称。**

基于容器技术，Kubernetes 可以方便的进行集群应用的部署、扩容、缩容、自愈机制、服务发现、负载均衡、日志、监控等功能，减少日常运维的工作量。

## 2. 组件
### 2.1 架构图
![image](https://www.zzcdev.cn/myblob/mdimg/clipboard.png)

### 2.2 核心组件
- **etcd：** 存储整个集群的状态
- **apiserver：** 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制
- **controller manager：** 维护集群的状态，故障检测、自动扩展、滚动更新等
- **scheduler：** 负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上
- **kubelet：** 负责维护容器的生命周期，同时负责 Volume（CVI）和网络（CNI）的管理
- **Container runtime：** 负责镜像管理以及Pod和容器的真正运行（CRI）
- **kube-proxy：** 负责为Service提供cluster内部的服务发现和负载均衡
- **kube-dns：** 负责为整个集群提供DNS服务
- **Ingress Controller：** 为服务提供外网入口


## 3. 资源
所有资源均可用此命令部署
```
kubectl apply -f 部署规范文件名.yml
```

### 3.1 Pod
Pod 是 Kubernetes 中能够创建和部署的最小单元，是 Kubernetes 集群中的一个应用实例，总是部署在同一个节点 Node 上。Pod中包含了一个或多个容器，还包括了存储、网络等各个容器共享的资源。
- 单容器Pod，最常见的应用方式。
- 多容器Pod，对于多容器Pod，Kubernetes会保证所有的容器都在同一台物理主机或虚拟主机中运行。多容器Pod是相对高阶的使用方式，除非应用耦合特别严重，一般不推荐使用这种方式。一个Pod内的容器共享IP地址和端口范围，容器之间可以通过 localhost 互相访问。

![image](https://www.zzcdev.cn/myblob/mdimg/clipboard01.png)


### 3.2 ReplicaSet
ReplicaSet 主要作用是确保Pod以你指定的副本数运行，即如果有容器异常退出，会自动创建新的 Pod 来替代；而异常多出来的容器也会自动回收。

### 3.3 Deployment
Deployment 提供了一种对 Pod 和 ReplicaSet 的管理方式，每一个 Deployment 都对应集群中的一次部署，是非常常见的 Kubernetes 对象。

**Deployment 部署规范详解**
```
# 必填，api 版本，可通过命令 kubectl api-versions 查看
apiVersion: apps/v1
# 必填，资源类型 Deployment
kind: Deployment
metadata:
  # 选填，自定义标签
  labels:
    app: award-backend-dev
  # 必填，Deployment 名称，全局唯一
  name: award-backend-dev
  # 选填，命名空间，默认为 default
  namespace: award-dev
# 定义 ReplicaSet
spec:
  # 选填，容器成功运行后，等待多长时间，判定容器为available状态
  minReadySeconds: 10
  # 选填，Deployment 控制器，等待600秒才能判定Deployment进程卡住
  progressDeadlineSeconds: 600
  # 必填，副本数/实例数
  replicas: 1
  # 选填，滚动更新时，保留旧副本集的最大数量，也是可回退的版本数量
  revisionHistoryLimit: 3
  # 必填，ReplicaSets(副本集/副本控制器) 所要管理的pod的匹配规则
  selector:
    # 1. label 匹配规则，与下方template.labels相对应，表示副本集所管理的pod带有app: award-backend-dev标签
    matchLabels:
      app: award-backend-dev
    # 2. 表达式匹配规则
    #matchExpressions:
      # key值为label的key，operator值为In、NotIn、Exists、DoesNotExis，values与label的value对应，通常用于匹配多个pod
      #- {key: env, operator: In, values: [dev]}
  # 必填，更新策略
  strategy:
    # 1. 滚动更新策略
    type: RollingUpdate
    # 滚动更新配置
    rollingUpdate:
      # 关闭旧pod前，先启动的新的pod的数量
      maxSurge: 1
      # 最大不可用的pod数量
      maxUnavailable: 1
    # 2. 重建更新策略
    #type: Recreate
  # 必填，定义 Pod
  template:
    metadata:
      # Pod 标签，用于 ReplicaSet 匹配
      labels:
        app: award-backend-dev
        env: dev
    spec:
      # 必填，定义pod中的容器
      containers:
      # 必填，镜像
      - image: harbor.pacific-textiles.com/award/award-backend-dev:53
        # 选填，镜像拉取策略，Always、Never、IfNotPresent，默认 IfNotPresent
        imagePullPolicy: Always
        # 选填，资源限制
        resources:
          limits:
            cpu: 500m   #容器启动后最多可用CPU核数
            memory: 400Mi  #容器启动最多可用内存数 单位MiB、GiB
          requests: #最低启动限制设置
            cpu: 250m  #最低容器启动可用CPU核数
            memory: 200Mi  #最低容器启动可用内存数
        # 必填，pod 名称
        name: award-backend-dev
        # 必填，容器端口声明
        ports:
          # 必填，容器端口
        - containerPort: 8928
          # 必填，名称
          name: http
          # 必填，协议
          protocol: TCP
          # 选填，定义了hostPort后，可以直接通过容器所在宿主机 IP 加端口访问，可以不通过 service
          #hostPort: 8999
      # 选填，节点选择器，用于控制pods调度到的节点，貌似不能与nodeName一起使用
      #nodeSelector:
        #deploy/role: dev #指定调度节点为带有 label  标记为：deploy/role=dev的node节点
      # 选填，Dns策略
      #dnsPolicy: ClusterFirst
      # 选填，拉取私有镜像时，使用的 Secret
      imagePullSecrets:
      - name: harbor
      # 选填，可以通过此配置指定/固定pod运行的节点，貌似不能与nodeSelector一起使用
      nodeName: mes-tomcat-1.pacific-textiles.com
      # 选填，容器重启策略，Always、on-failure、Never，默认on-failure
      restartPolicy: Always
```


### 3.4 Service
在 Kubernetes 中，pod通常需要对来自集群内部的其他pod或来自集群外部的客户端的HTTP请求做出响应。
- pod是短暂的一—它们随时会启动或者关闭，无论是为了给其他pod提供空间而从节点中被移除，或者是减少了pod的数量，又或者是因为节点异常。
- Kubernetes在pod启动前会给已经调度到节点上的pod分配IP地址一—因此客户端不能提前知道提供服务pod的IP地址。
- 水平伸缩意味着多个pod可能会提供相同的服务一—每个pod都有自己的IP地址，客户端无须关心后端提供服务pod的数量，以及各自对应的IP地址。它们无须记录每个pod的IP地址。相反，所有的pod可以通过一个单一的IP地址进行访问。
　　

为了解决上述问题，Kubernetes提供了一种资源类型——服务（Service)。

**Service 部署规范**
```
apiVersion: v1
# 必填，资源类型 Deployment
kind: Service
metadata:
  # 必填，Service 名，全局唯一
  name: award-backend-dev
  # 选填，命名空间，默认 default，需要与 Deployment 在同一命名空间
  namespace: award-dev
spec:
  # 选填，集群内部 IP，用于集群内，服务间相互访问，默认自动分配，可以写死
  # 但必须为 Kubernetes 集群搭建时所配置的 service_cluster_ip_range 子网范围内
  clusterIP: 100.101.183.233
  # 选填，使用 NodePode 类型的 Service 时，集群上所有节点的 IP 都能访问到此服务
  # 但并不是每台机器上都会有对应的 Pod，这会导致通过没有 Pod 的机器访问时，客户端的源IP地址丢失了
  # 将此参数设置为 Local 可以解决此问题，默认为 Cluster
  externalTrafficPolicy: Cluster
  # 必填，端口映射，可以映射多个
  ports:
  # 必填，自定义映射名
  - name: http
    # 选填，nodePort 端口
    nodePort: 8000
    # 必填，集群中服务间相互访问时使用的端口
    port: 80
    # 必填，协议类型
    protocol: TCP
    # 必填，容器自身的端口
    targetPort: 80
  # 必填，选择器，用于匹配和管理对应的 Pod，与 Deployment 中 spec.template.metadata.labels 相匹配
  selector:
    app: award-backend-dev
  # 选填，session 保持配置，默认为 None，可通过配置为如下实现 session 保持
  # sessionAffinity: ClientIP
  # sessionAffinityConfig:
  #   clientIP:
  #     timeoutSeconds: 10800  # session 保持时间，单位为秒
  sessionAffinity: None
  # 选填，默认为 ClusterIP
  # 三种类型：
  # ClusterIP：默认，分配一个集群内部可以访问的虚拟IP；
  # NodePort：在每个Node上分配一个端口作为外部访问入口；
  # LoadBalancer：工作在特定的Cloud Provider上，例如Google Cloud，AWS，OpenStack。
  type: NodePort
```

### 3.5 Ingress
Ingress 是一个 API 对象，用来管理集群外部访问集群内部的服务（主要为http和https）。Ingress 可以提供负载均衡、ssl 卸载和虚拟主机的功能。可以理解 service 是4层负载均衡，ingress 是7层负载均衡。

**Ingress 部署规范**
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: award-backend-dev-ingress
  namespace: award-dev
spec:
  rules:
    # 域名，域名绑定集群任一 IP ，即可通过此域名访问服务
  - host: awarddev.*.com
    http:
      paths:
      - backend:
          serviceName: award-frontend-dev
          servicePort: 80
```

### 3.6 HPA（HorizontalPodAutoscaler）
HPA全称Horizontal Pod Autoscaling，即pod的水平自动扩展，针对于实例数目的增减。

**HPA 部署规范**
```
apiVersion: autoscaling/v2beta1
# 必填，资源类型，HPA
kind: HorizontalPodAutoscaler
metadata:
  # 必填，HPA 名称，全局唯一
  name: award-frontend-dev
  # 选填，命名空间，默认 default，必须与 Deployment 一致
  namespace: myapp
spec:
  # 必填，伸缩目标
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tomcat-demo
  # 最大副本数
  maxReplicas: 3
  # 最小副本数
  minReplicas: 1
  metrics:
  # 必填，类型有三种，Resource、Pods、Object、External
  - type: Resource
    resource:
      # 根据 cpu 占用伸缩
      name: cpu
      # 目标阈值设定65%，大于则增加副本数，小于则减少副本数
      targetAverageUtilization: 65
  - type: Resource
    resource:
      # 根据内存占用伸缩
      name: memory
      targetAverageUtilization: 65
  - type: Pods
    pods:
      metricName: packets-per-second
      targetAverageValue: 1k   # 每秒数据量
  - type: Object
    metricName: requests-per-second
      target:
        apiVersion: extensions/v1beta1
        kind: Ingress
        name: main-route
      targetValue: 10k   # 每秒请求量
  - type: External
    external:
      metric:
        name: queue_messages_ready
        # 该字段与第三方的指标标签相关联
        selector:
          matchLabels:
            env: "stage"
            app: "myapp"
      # External指标类型下只支持Value和AverageValue类型的目标值
      target:
        type: AverageValue
        averageValue: 30
```

### 3.7 Secret
Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。

三种类型：
- **Opaque：** 使用base64编码存储信息，可以通过base64 --decode解码获得原始数据，因此安全性弱。
- **kubernetes.io/dockerconfigjson：** 用于存储docker registry的认证信息。
- **kubernetes.io/service-account-token：** 用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。

三种创建方式：
- **docker-registry：** 用于存储docker私有仓库的账号密码信息
- **generic：** 用于存储数据库密码等需要加密的信息
- **tls：** 用于证书使用

```
使用命令创建
echo -n 'admin' > ./username.txt
echo -n '123456' > ./password.txt

kubectl -n <namespace> create secret generic <secret-name> --from-file=./username.txt --from-file=./password.txt

kubectl -n <namespace> create secret generic <secret-name> --from-literal=<key>=<value>

kubectl -n <namespace> create secret docker-registry <secret-name> -n <namespace> \
    --docker-server=<registry-url> \
    --docker-username=<username> \
    --docker-password=<password> \
    --docker-email=<your-email>

kubectl -n <namespace> create secret tls <secret-name> --cert=path/to/tls.cert --key=path/to/tls.key
```

```
使用部署规范 secret.yml 创建，必须为 base64 编码格式
$ echo -n "admin" | base64
YWRtaW4=
$ echo -n "123456" | base64
MTIzNDU2

文件内容：
apiVersion: v1
kind: Secret
metadata:
  name: <secret-name>
  namespace: <namespace>
type: Opaque
data:
  password: MTIzNDU2
  username: YWRtaW4=

创建
kubectl create -f secret.yml
```

### 3.8 ConfigMap
Kubernetes 中提供了 ConfigMap 来实现向容器中提供配置文件或环境变量来实现不同配置，从而实现了镜像配置与镜像本身解耦，使容器应用做到不依赖于环境配置。
```
使用命令创建
kubectl -n <namespace> create configmap <configmap-name> --from-literal=key1=value1 --from-literal=key2=value2
```

```
使用部署规范 config.yml 创建

文件内容：
apiVersion: v1
kind: ConfigMap
metadata:
  name: <ConfigMap-name>
  namespace: <namespace>
data:
  key1: value1
  key2: value2

创建
kubectl create -f config.yaml
```

在资源中使用例子
```
kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm
kubectl create configmap env-config --from-literal=log_level=INFO

定义 POD 规范：
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      # 在 command 中引用
      command: [ "/bin/sh", "-c", "echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never

运行的容器内环境变量为
SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
```

### 3.9 其他
资源类型 | 功能
-- | --
Endpoints | 将外部服务器映射为 kubernetes 内部的一个服务
StatefulSet | Deployment 的一种变体，管理所有有状态的服务
ServiceAccount | 为 Pod 中的进程和外部用户提供身份信息
PersistentVolume | PV 持久卷，由管理员提供的网络存储的一部分
PersistentVolumeClaim | PVC 持久卷声明，是用户的一种存储请求
Job | 非耐久性任务，任务完成后，Pod 结束运行
CronJob | 用来管理基于时间的 Job


## 4. 集群搭建（RKE）

- **RKE 中文文档** https://docs.rancher.cn/rke/
- **RKE 英文文档** https://rancher.com/docs/rke/latest/en/

### 4.1 下载地址
```
https://github.com/rancher/rke/releases

下载 rke_linux-amd64
修改名称 mv rke_linux-amd64 rke
执行权  chmod +x rke
```

### 4.2 集群配置
```
vi cluster.yml
```
```
# 定义节点，注意需要提前交换公钥
nodes:
  - address: rancher-node1
    user: rancher
    # controlplane、etcd 为 Master 节点角色，worker 为 slave 节点角色
    # 主节点数量为奇数个，容错为 (n-1)/2，若超过容错数量，可能会导致无法访问，重启宕机节点可自动恢复
    role: [controlplane,etcd,worker]
  - address: rancher-node2
    user: rancher
    role: [controlplane,etcd,worker]
  - address: rancher-node3
    user: rancher
    role: [controlplane,etcd,worker]
  - address: rancher-node4
    user: rancher
    role: [worker]
  - address: rancher-node5
    user: rancher
    role: [worker]
  - address: rancher-node6
    user: rancher
    role: [worker]

# 集群名称
cluster_name: k8s
# 忽略 docker 版本
ignore_docker_version: true

# 私有仓库认证
private_registries:
  - url: harbor.**.com
    user: admin
    password: admin

# 核心组件配置
services:
  kube-api:
    # Pod 的网段
    service_cluster_ip_range: 100.0.0.0/16
    # nodePort 端口
    service_node_port_range: 1-65535
  kube-controller:
    # 集群 的网段
    cluster_cidr: 100.1.0.0/16
    # Service 的网段
    service_cluster_ip_range: 100.1.0.0/16
  kubelet:
    # kube-dns 的IP
    cluster_dns_server: 100.0.0.10
    extra_binds:
      - "/lib/modules:/lib/modules"

ingress:
  provider: nginx
  options:
    # ingress-nginx后端pod容器获取客服端真实ip
    use-forwarded-headers: 'true'
```

### 4.3 完整示例配置
```
nodes:
  - address: 1.1.1.1
    internal_address:
    user: ubuntu
    role:
      - controlplane
      - etcd
    ssh_key_path: /home/user/.ssh/id_rsa
    port: 2222
  - address: 2.2.2.2
    internal_address:
    user: ubuntu
    role:
      - worker
    ssh_key: |-
      -----BEGIN RSA PRIVATE KEY-----
      -----END RSA PRIVATE KEY-----
  - address: example.com
    internal_address:
    user: ubuntu
    role:
      - worker
    hostname_override: node3
    internal_address: 192.168.1.6
    labels:
      app: ingress
      app: dns

# 如果设置为true，则可以使用不受支持的Docker版本
ignore_docker_version: false

# 集群等级的SSH私钥(private key)
## 如果节点未配置SSH私钥，RKE将会以此私钥去连接集群节点
ssh_key_path: ~/.ssh/test

# 使用SSH agent来提供SSH私钥
## 需要配置环境变量`SSH_AUTH_SOCK`指向已添加私钥的SSH agent
ssh_agent_auth: false

# 配置docker root目录
docker_root_dir: "/var/lib/docker"

# 私有仓库
## 当设置`is_default: true`后，构建集群时会自动在配置的私有仓库中拉取镜像
## 如果使用的是DockerHub镜像仓库，则可以省略`url`或将其设置为`docker.io`
## 如果使用内部公开仓库，则可以不用设置用户名和密码

private_registries:
  - url: registry.com
    user: Username
    password: password
    is_default: true

# 堡垒机
## 如果集群节点需要通过堡垒机跳转，那么需要为RKE配置堡垒机信息
bastion_host:
  address: x.x.x.x
  user: ubuntu
  port: 22
  ssh_key_path: /home/user/.ssh/bastion_rsa
# or
#   ssh_key: |-
#     -----BEGIN RSA PRIVATE KEY-----
#
#     -----END RSA PRIVATE KEY-----

# 设置Kubernetes集群名称

# 定义kubernetes版本.
## 目前, 版本定义需要与rancher/types defaults map相匹配: https://github.com/rancher/types/blob/master/apis/management.cattle.io/v3/k8s_defaults.go#L14 （后期版本请查看: https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go ）
## 如果同时定义了kubernetes_version和system_images中的kubernetes镜像，则system_images配置将优先于kubernetes_version
kubernetes_version: v1.14.3-rancher1

# `system_images`优先级更高，如果没有单独指定`system_images`镜像，则会使用`kubernetes_version`对应的默认镜像版本。
## 默认Tags: https://github.com/rancher/types/blob/master/apis/management.cattle.io/v3/k8s_defaults.go)(Rancher v2.3或者RKE v0.3之后的版本请查看: https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go ）
system_images:
  etcd: rancher/coreos-etcd:v3.3.10-rancher1
  alpine: rancher/rke-tools:v0.1.34
  nginx_proxy: rancher/rke-tools:v0.1.34
  cert_downloader: rancher/rke-tools:v0.1.34
  kubernetes_services_sidecar: rancher/rke-tools:v0.1.34
  kubedns: rancher/k8s-dns-kube-dns:1.15.0
  dnsmasq: rancher/k8s-dns-dnsmasq-nanny:1.15.0
  kubedns_sidecar: rancher/k8s-dns-sidecar:1.15.0
  kubedns_autoscaler: rancher/cluster-proportional-autoscaler:1.3.0
  coredns: rancher/coredns-coredns:1.3.1
  coredns_autoscaler: rancher/cluster-proportional-autoscaler:1.3.0
  kubernetes: rancher/hyperkube:v1.14.3-rancher1
  flannel: rancher/coreos-flannel:v0.10.0-rancher1
  flannel_cni: rancher/flannel-cni:v0.3.0-rancher1
  calico_node: rancher/calico-node:v3.4.0
  calico_cni: rancher/calico-cni:v3.4.0
  calico_controllers: ""
  calico_ctl: rancher/calico-ctl:v2.0.0
  canal_node: rancher/calico-node:v3.4.0
  canal_cni: rancher/calico-cni:v3.4.0
  canal_flannel: rancher/coreos-flannel:v0.10.0
  weave_node: weaveworks/weave-kube:2.5.0
  weave_cni: weaveworks/weave-npc:2.5.0
  pod_infra_container: rancher/pause:3.1
  ingress: rancher/nginx-ingress-controller:0.21.0-rancher3
  ingress_backend: rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1
  metrics_server: rancher/metrics-server:v0.3.1

services:
  etcd:
    # if external etcd is used
    # path: /etcdcluster
    # external_urls:
    #   - https://etcd-example.com:2379
    # ca_cert: |-
    #   -----BEGIN CERTIFICATE-----
    #   xxxxxxxxxx
    #   -----END CERTIFICATE-----
    # cert: |-
    #   -----BEGIN CERTIFICATE-----
    #   xxxxxxxxxx
    #   -----END CERTIFICATE-----
    # key: |-
    #   -----BEGIN PRIVATE KEY-----
    #   xxxxxxxxxx
    #   -----END PRIVATE KEY-----
    # Rancher 2用户注意事项：如果在创建Rancher Launched Kubernetes时使用配置文件配置集群，则`kube_api`服务名称应仅包含下划线。这仅适用于Rancher v2.0.5和v2.0.6。
    # 以下参数仅支持RKE部署的etcd集群

    # 开启自动备份
    ## rke版本小于0.2.x或rancher版本小于v2.2.0时使用
    snapshot: true
    creation: 5m0s
    retention: 24h
    ## rke版本大于等于0.2.x或rancher版本大于等于v2.2.0时使用(两段配置二选一)
    backup_config:
      enabled: true           # 设置true启用ETCD自动备份，设置false禁用；
      interval_hours: 12      # 快照创建间隔时间，不加此参数，默认5分钟；
      retention: 6            # etcd备份保留份数；
      # S3配置选项
      s3backupconfig:
        access_key: "myaccesskey"
        secret_key:  "myaccesssecret"
        bucket_name: "my-backup-bucket"
        folder: "folder-name" # 此参数v2.3.0之后可用
        endpoint: "s3.eu-west-1.amazonaws.com"
        region: "eu-west-1"
    # 扩展参数
    extra_args:
      auto-compaction-retention: 240 #(单位小时)
      # 修改空间配额为$((6*1024*1024*1024))，默认2G,最大8G
      quota-backend-bytes: '6442450944'
  kube-api:
    # cluster_ip范围
    ## 这必须与kube-controller中的service_cluster_ip_range匹配
    service_cluster_ip_range: 10.43.0.0/16
    # NodePort映射的端口范围
    service_node_port_range: 30000-32767
    # Pod安全策略
    pod_security_policy: false
    # kubernetes API server扩展参数
    ## 这些参数将会替换默认值
    extra_args:
      watch-cache: true
      default-watch-cache-size: 1500
      # 事件保留时间，默认1小时
      event-ttl: 1h0m0s
      # 默认值400，设置0为不限制，一般来说，每25~30个Pod有15个并行
      max-requests-inflight: 800
      # 默认值200，设置0为不限制
      max-mutating-requests-inflight: 400
      # kubelet操作超时，默认5s
      kubelet-timeout: 5s
      # 启用审计日志到标准输出
      audit-log-path: "-"
      # 增加删除workers的数量
      delete-collection-workers: 3
      # 将日志输出的级别设置为debug模式
      v: 4
  # Rancher 2用户注意事项：如果在创建Rancher Launched Kubernetes时使用配置文件配置集群，则`kube_controller`服务名称应仅包含下划线。这仅适用于Rancher v2.0.5和v2.0.6。
  kube-controller:
    # Pods_ip范围
    cluster_cidr: 10.42.0.0/16
    # cluster_ip范围
    ## 这必须与kube-api中的service_cluster_ip_range相同
    service_cluster_ip_range: 10.43.0.0/16
    extra_args:
      # 修改每个节点子网大小(cidr掩码长度)，默认为24，可用IP为254个；23，可用IP为510个；22，可用IP为1022个；
      node-cidr-mask-size: '24'

      feature-gates: "TaintBasedEvictions=false"
      # 控制器定时与节点通信以检查通信是否正常，周期默认5s
      node-monitor-period: '5s'
      ## 当节点通信失败后，再等一段时间kubernetes判定节点为notready状态。
      ## 这个时间段必须是kubelet的nodeStatusUpdateFrequency(默认10s)的整数倍，
      ## 其中N表示允许kubelet同步节点状态的重试次数，默认40s。
      node-monitor-grace-period: '20s'
      ## 再持续通信失败一段时间后，kubernetes判定节点为unhealthy状态，默认1m0s。
      node-startup-grace-period: '30s'
      ## 再持续失联一段时间，kubernetes开始迁移失联节点的Pod，默认5m0s。
      pod-eviction-timeout: '1m'

      # 默认5. 同时同步的deployment的数量。
      concurrent-deployment-syncs: 5
      # 默认5. 同时同步的endpoint的数量。
      concurrent-endpoint-syncs: 5
      # 默认20. 同时同步的垃圾收集器工作器的数量。
      concurrent-gc-syncs: 20
      # 默认10. 同时同步的命名空间的数量。
      concurrent-namespace-syncs: 10
      # 默认5. 同时同步的副本集的数量。
      concurrent-replicaset-syncs: 5
      # 默认5m0s. 同时同步的资源配额数。（新版本中已弃用）
      # concurrent-resource-quota-syncs: 5m0s
      # 默认1. 同时同步的服务数。
      concurrent-service-syncs: 1
      # 默认5. 同时同步的服务帐户令牌数。
      concurrent-serviceaccount-token-syncs: 5
      # 默认5. 同时同步的复制控制器的数量
      concurrent-rc-syncs: 5
      # 默认30s. 同步deployment的周期。
      deployment-controller-sync-period: 30s
      # 默认15s。同步PV和PVC的周期。
      pvclaimbinder-sync-period: 15s


  kubelet:
    # 集群搜索域
    cluster_domain: cluster.local
    # 内部DNS服务器地址
    cluster_dns_server: 10.43.0.10
    # 禁用swap
    fail_swap_on: false
    # 扩展变量
    extra_args:
      root-dir:  "/var/lib/kubelet"
      docker-root: "/var/lib/docker"
      feature-gates: "TaintBasedEvictions=false"
      # 指定pause镜像
      pod-infra-container-image: 'rancher/pause:3.1'
      # 传递给网络插件的MTU值，以覆盖默认值，设置为0(零)则使用默认的1460
      network-plugin-mtu: '1500'
      # 修改节点最大Pod数量
      max-pods: "250"
      # 密文和配置映射同步时间，默认1分钟
      sync-frequency: '3s'
      # Kubelet进程可以打开的文件数（默认1000000）,根据节点配置情况调整
      max-open-files: '2000000'
      # 与apiserver会话时的并发数，默认是10
      kube-api-burst: '30'
      # 与apiserver会话时的 QPS,默认是5，QPS = 并发量/平均响应时间
      kube-api-qps: '15'
      # kubelet默认一次拉取一个镜像，设置为false可以同时拉取多个镜像，
      # 前提是存储驱动要为overlay2，对应的Dokcer也需要增加下载并发数，参考[docker配置](/rancher2x/install-prepare/best-practices/docker/)
      serialize-image-pulls: 'false'
      # 拉取镜像的最大并发数，registry-burst不能超过registry-qps ，
      # 仅当registry-qps大于0(零)时生效，(默认10)。如果registry-qps为0则不限制(默认5)。
      registry-burst: '10'
      registry-qps: '0'
      cgroups-per-qos: 'true'
      cgroup-driver: 'cgroupfs'

      # 节点资源预留
      enforce-node-allocatable: 'pods'
      system-reserved: 'cpu=0.25,memory=200Mi'
      kube-reserved: 'cpu=0.25,memory=1500Mi'
      # POD驱逐，这个参数只支持内存和磁盘。
      ## 硬驱逐伐值
      ### 当节点上的可用资源降至保留值以下时，就会触发强制驱逐。强制驱逐会强制kill掉POD，不会等POD自动退出。
      eviction-hard: 'memory.available<300Mi,nodefs.available<10%,imagefs.available<15%,nodefs.inodesFree<5%'
      ## 软驱逐伐值
      ### 以下四个参数配套使用，当节点上的可用资源少于这个值时但大于硬驱逐伐值时候，会等待eviction-soft-grace-period设置的时长；
      ### 等待中每10s检查一次，当最后一次检查还触发了软驱逐伐值就会开始驱逐，驱逐不会直接Kill POD，先发送停止信号给POD，然后等待eviction-max-pod-grace-period设置的时长；
      ### 在eviction-max-pod-grace-period时长之后，如果POD还未退出则发送强制kill POD"
      eviction-soft: 'memory.available<500Mi,nodefs.available<50%,imagefs.available<50%,nodefs.inodesFree<10%'
      eviction-soft-grace-period: 'memory.available=1m30s'
      eviction-max-pod-grace-period: '30'
      eviction-pressure-transition-period: '30s'
      # 指定kubelet多长时间向master发布一次节点状态。注意: 它必须与kube-controller中的nodeMonitorGracePeriod一起协调工作。(默认 10s)
      node-status-update-frequency: 10s
      # 设置cAdvisor全局的采集行为的时间间隔，主要通过内核事件来发现新容器的产生。默认1m0s
      global-housekeeping-interval: 1m0s
      # 每个已发现的容器的数据采集频率。默认10s
      housekeeping-interval: 10s
      # 所有运行时请求的超时，除了长时间运行的 pull, logs, exec and attach。超时后，kubelet将取消请求，抛出错误，然后重试。(默认2m0s)
      runtime-request-timeout: 2m0s
      # 指定kubelet计算和缓存所有pod和卷的卷磁盘使用量的间隔。默认为1m0s
      volume-stats-agg-period: 1m0s

    # 可以选择定义额外的卷绑定到服务
    extra_binds:
      - "/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins"
      - "/etc/iscsi:/etc/iscsi"
      - "/sbin/iscsiadm:/sbin/iscsiadm"
  kubeproxy:
    extra_args:
      # 默认使用iptables进行数据转发，如果要启用ipvs，则此处设置为`ipvs`
      proxy-mode: ""
      # 与kubernetes apiserver通信并发数,默认10;
      kube-api-burst: 20
      # 与kubernetes apiserver通信时使用QPS，默认值5，QPS = 并发量/平均响应时间
      kube-api-qps: 10
    extra_binds:
      - "/lib/modules:/lib/modules"
  scheduler:
    extra_args: {}
    extra_binds: []
    extra_env: []

# 目前，只支持x509验证
## 您可以选择创建额外的SAN(主机名或IP)以添加到API服务器PKI证书。
## 如果要为control plane servers使用负载均衡器，这很有用。
authentication:
  strategy: "x509|webhook"
  webhook:
    config_file: "...."
    cache_timeout: 5s
  sans:
    # 此处配置备用域名或IP，当主域名或者IP无法访问时，可通过备用域名或IP访问
    - "192.168.1.100"
    - "www.test.com"
# Kubernetes认证模式
## Use `mode: rbac` 启用 RBAC
## Use `mode: none` 禁用 认证
authorization:
  mode: rbac
# 如果要设置Kubernetes云提供商，需要指定名称和配置，非云主机则留空；
cloud_provider:
# Add-ons是通过kubernetes jobs来部署。 在超时后，RKE将放弃重试获取job状态。以秒为单位。
addon_job_timeout: 30
# 有几个网络插件可以选择：`flannel、canal、calico`，Rancher2默认canal
network:
  plugin: canal
  options:
    flannel_backend_type: "vxlan"
# 目前只支持nginx ingress controller
## 可以设置`provider: none`来禁用ingress controller
ingress:
  provider: nginx
  node_selector:
    app: ingress
# 配置dns上游dns服务器
## 可用rke版本 v0.2.0
dns:
  provider: coredns
  upstreamnameservers:
  - 114.114.114.114
  - 1.2.4.8
  node_selector:
    app: dns
# 安装附加应用
## 所有附加应用都必须指定命名空间
addons: |-
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      namespace: default
    spec:
      containers:
        image: nginx
        ports:
        - containerPort: 80

addons_include:
    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-operator.yml
    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-cluster.yml
    - /path/to/manifest
```

### 4.4 开放相应端口
**RKE node**
协议 | 端口 | 源 | 目标
-- | -- | -- | --
TCP | 22 | RKE node | 配置文件中所有节点
TCP | 6443 | RKE node |  controlplane 角色节点


**etcd nodes 入站端口**
协议 | 端口 | 源
-- | -- | -- | --
TCP | 2376 | Rancher 节点
TCP	| 2379 | etcd 节点和 controlplane 节点
TCP	| 2380 | etcd 节点和 controlplane 节点
UDP	| 8472 | etcd 节点和 controlplane 节点
TCP	| 9099 | etcd 节点
TCP	| 10250	| controlplane 节点


**etcd nodes 出站端口**
协议 | 端口 | 目标
-- | -- | -- | --
TCP | 443 | Rancher 节点
TCP	| 2379 | etcd 节点
TCP	| 2380 | etcd 节点
UDP	| 6443 | controlplane 节点
TCP	| 8472 | etcd 节点和 controlplane 节点
TCP	| 9099 | etcd 节点


**controlplane nodes 入站端口**
协议 | 端口 | 源
-- | -- | -- | --
TCP	| 80 | 所有节点
TCP | 443 | 所有节点
TCP	| 2376 | Rancher 节点
TCP	| 6443 | etcd 节点和 controlplane 节点
UDP	| 8472 | etcd 节点和 controlplane 节点
TCP	| 9099 | controlplane 节点
TCP	| 10250 | controlplane 节点
TCP	| 10254 | controlplane 节点
TCP/UDP | 1-65535 | Service 使用的端口，用到时开放


**controlplane nodes 出站端口**
协议 | 端口 | 目标
-- | -- | -- | --
TCP	| 443 | rancher 节点
TCP | 2379 | etcd 节点
TCP	| 2380 | etcd 节点
TCP	| 8472 | 所有节点
UDP	| 9099 | controlplane 节点
TCP	| 10250 | etcd 节点和 controlplane 节点
TCP	| 10254 | controlplane 节点


**worker nodes 入站端口**
协议 | 端口 | 源
-- | -- | -- | --
TCP | 22 | SSH 端口
TCP	| 3389 | Windows 远程控制端口
TCP	| 80 | 所有节点
TCP	| 443 | 所有节点
TCP | 2376 | Rancher 节点
UDP	| 8472 | etcd 节点和 controlplane 节点
TCP	| 9099 | worker 节点
TCP	| 10250 | controlplane 节点
TCP	| 10254 | worker 节点
TCP/UDP | 1-65535 | Service 使用的端口，用到时开放


**worker nodes 出站端口**
协议 | 端口 | 源
-- | -- | -- | --
TCP | 443 | rancher 节点
TCP	| 8443 | controlplane 节点
UDP	| 8472 | etcd 节点和 controlplane 节点
TCP	| 9099 | worker 节点
TCP	| 10254 | worker 节点
TCP/UDP | 1-65535 | Service 使用的端口，用到时开放



### 4.5 安装集群
```
./rke up --config=cluster.yml
```


### 4.6 安装和配置 kubectl
```
安装
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
chmod +x kubectl
mv kubectl /usr/bin/kubectl

配置
rke up 命令执行完后，当前目录下会生成 kube_config_rancher-cluster.yml 文件
cp kube_config_rancher-cluster.yml ~/.kube/config

测试
kubectl get node
kubectl get service
```

## 5. 安装 Rancher
- **Rancher 中文文档** https://docs.rancher.cn/rancher2x/
- **Rancher 英文文档** https://rancher.com/docs/rancher/v2.x/en/

helm HTTP L7 负载安装方式
### 5.1 helm 简介
Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的 apt、Centos 中使用的 yum 或者 Python 中的 pip 一样，能快速查找、下载和安装软件包。

### 5.2 helm 安装

```
下载地址
https://github.com/helm/helm/releases

下载 Linux amd64 版本，解压并拷贝
tar -xf helm-v3.1.0-linux-amd64.tar.gz
cp helm-v3.1.0/helm /usr/bin/helm
```

### 5.3 使用 helm 安装 rancher
```
helm repo add rancher-stable \
https://releases.rancher.com/server-charts/stable

helm install rancher rancher-stable/rancher \
    --namespace cattle-system \
    --set hostname=<您自己的域名> \
    --set tls=external
```

注：使用自签名证书请参考
```
https://docs.rancher.cn/rancher2x/installation/helm-ha-install/online/https-l7.html#_5-2-%E9%85%8D%E7%BD%AEssl%E5%B9%B6%E5%AE%89%E8%A3%85rancher-server

https://docs.rancher.cn/rancher2x/install-prepare/self-signed-ssl.html#_4-%E7%94%9F%E6%88%90%E8%87%AA%E7%AD%BE%E5%90%8D%E8%AF%81%E4%B9%A6
```

### 5.4 安装和配置 nginx
```
yum install -y nginx

vi /etc/nginx/conf.d/rancher.conf
```
```
worker_processes 4;
worker_rlimit_nofile 40000;

events {
    worker_connections 8192;
}

http {
    upstream rancher {
        server IP_NODE_1:80;
        server IP_NODE_2:80;
        server IP_NODE_3:80;
    }

    map $http_upgrade $connection_upgrade {
        default Upgrade;
        ''      close;
    }

    server {
        listen 443 ssl http2; # 如果是升级或者全新安装v2.2.2,需要禁止http2，其他版本不需修改。
        server_name 域名;
        ssl_certificate <更换证书>;
        ssl_certificate_key <更换证书私钥>;

        location / {
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Port $server_port;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://rancher;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $connection_upgrade;
            # This allows the ability for the execute shell window to remain open for up to 15 minutes.
            ## Without this parameter, the default is 1 minute and will automatically close.
            proxy_read_timeout 900s;
            proxy_buffering off;
        }
    }

    server {
        listen 80;
        server_name 域名;
        return 301 https://$server_name$request_uri;
    }
}
```

### 5.5 域名映射 IP
若没有 DNS 服务器，需要执行以下两条命令，以映射对应的 nginx 服务器域名
```
kubectl -n cattle-system patch  deployments cattle-cluster-agent --patch '{
    "spec": {
        "template": {
            "spec": {
                "hostAliases": [
                    {
                        "hostnames":
                        [
                            "<hostname/域名>"
                        ],
                            "ip": "<ip>"
                    }
                ]
            }
        }
    }
}'


kubectl -n cattle-system patch  daemonsets cattle-node-agent --patch '{
    "spec": {
        "template": {
            "spec": {
                "hostAliases": [
                    {
                        "hostnames":
                        [
                            "<hostname/域名>"
                        ],
                            "ip": "<ip>"
                    }
                ]
            }
        }
    }
}'
```

## 6. kubectl 集群管理命令
### 6.1 基本
#### （1）查看集群信息
```
kubectl cluster-info
kubectl cluster-info dump
```

#### （2）组件信息
```
kubectl get componentstatuses
```

#### （3）Api 版本
```
kubectl api-versions
```

#### （4）资源和组
```
kubectl api-resources
```

### 6.2 结点管理
#### （1）结点列表
```
kubectl get node(s)
可加 -o wide/yaml 设置显示格式
```

#### （2）结点详细
```
kubectl describe node <node-name>
```


### 6.3 资源管理
包括：pod、deployment、service、job、ingress等。
#### （1）列表
```
kubectl get namespaces
kubectl get pods
kubectl get rc                 # rc为副本控制器
kubectl get service
kubectl get ...

默认获取的是 default 命名空间，可用 -n 指定命名空间
kubectl -n dev get pods
```

#### （2）详细
```
kubectl describe pod <Pod-name>
kubectl describe service <Service-name>
kubectl describe deployment <Deployment-name>

默认获取的是 default 命名空间，可用 -n 指定命名空间
kubectl -n dev describe pod <Pod-name>
```

#### （3）创建资源
```
kubectl create namespace <namespace-name>
kubectl create -f <yml文件>
```

#### （4）删除服务
```
kubectl delete -f <yml文件>
kubectl delete pod <pod-name>
kubectl delete deployment <deployment-name>
```
